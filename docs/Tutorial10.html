<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Tutorial_10</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">DACSS758 Text as Data Fall 2023</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial1.html" rel="" target="">
 <span class="menu-text">Tutorial 1 Introduction to R</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial2.html" rel="" target="">
 <span class="menu-text">Tutorial 2 Text as Data</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial3.html" rel="" target="">
 <span class="menu-text">Tutorial 3 Web Scraping in R</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial4.html" rel="" target="">
 <span class="menu-text">Tutorial 4 Natural Language Processing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial5.html" rel="" target="">
 <span class="menu-text">Tutorial 5 Preprocessing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial6.html" rel="" target="">
 <span class="menu-text">Tutorial 6 Text Representation I</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial7.html" rel="" target="">
 <span class="menu-text">Tutorial7_WordEmbeddings/Text Representation II</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial8.html" rel="" target="">
 <span class="menu-text">Tutorial8_Dictionary/Sentiment Analysis</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Tutorial9.html" rel="" target="">
 <span class="menu-text">Tutorial9_Supervised Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./Tutorial10.html" rel="" target="" aria-current="page">
 <span class="menu-text">Tutorial10_Topic Models</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#k-means" id="toc-k-means" class="nav-link active" data-scroll-target="#k-means">K-Means</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#front-end-matters" id="toc-front-end-matters" class="nav-link" data-scroll-target="#front-end-matters">Front-end Matters</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#text-representation" id="toc-text-representation" class="nav-link" data-scroll-target="#text-representation">Text Representation</a></li>
  <li><a href="#running-the-clustering-algorithms" id="toc-running-the-clustering-algorithms" class="nav-link" data-scroll-target="#running-the-clustering-algorithms">Running the clustering algorithms</a>
  <ul class="collapse">
  <li><a href="#k-means-1" id="toc-k-means-1" class="nav-link" data-scroll-target="#k-means-1">K-means</a></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering">Hierarchical clustering</a></li>
  <li><a href="#plotting" id="toc-plotting" class="nav-link" data-scroll-target="#plotting">Plotting</a></li>
  </ul></li>
  <li><a href="#determining-k" id="toc-determining-k" class="nav-link" data-scroll-target="#determining-k">Determining K</a></li>
  </ul></li>
  <li><a href="#topic-models" id="toc-topic-models" class="nav-link" data-scroll-target="#topic-models">Topic Models</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1">Introduction</a></li>
  <li><a href="#latent-dirichlet-allocation" id="toc-latent-dirichlet-allocation" class="nav-link" data-scroll-target="#latent-dirichlet-allocation">Latent Dirichlet Allocation</a>
  <ul class="collapse">
  <li><a href="#front-end-matter" id="toc-front-end-matter" class="nav-link" data-scroll-target="#front-end-matter">Front-end Matter</a></li>
  <li><a href="#vectorization" id="toc-vectorization" class="nav-link" data-scroll-target="#vectorization">Vectorization</a></li>
  <li><a href="#vocabulary-based-vectorization" id="toc-vocabulary-based-vectorization" class="nav-link" data-scroll-target="#vocabulary-based-vectorization">Vocabulary-based Vectorization</a></li>
  <li><a href="#fitting" id="toc-fitting" class="nav-link" data-scroll-target="#fitting">Fitting</a></li>
  <li><a href="#describing-topics-top-words" id="toc-describing-topics-top-words" class="nav-link" data-scroll-target="#describing-topics-top-words">Describing Topics: Top Words</a></li>
  <li><a href="#apply-learned-model-to-new-data" id="toc-apply-learned-model-to-new-data" class="nav-link" data-scroll-target="#apply-learned-model-to-new-data">Apply Learned Model to New Data</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">Visualization</a></li>
  </ul></li>
  <li><a href="#structural-topic-model" id="toc-structural-topic-model" class="nav-link" data-scroll-target="#structural-topic-model">Structural Topic Model</a>
  <ul class="collapse">
  <li><a href="#front-end-matters-1" id="toc-front-end-matters-1" class="nav-link" data-scroll-target="#front-end-matters-1">Front-end Matters</a></li>
  <li><a href="#creating-the-dfm" id="toc-creating-the-dfm" class="nav-link" data-scroll-target="#creating-the-dfm">Creating the DFM</a></li>
  <li><a href="#correlated-topic-model" id="toc-correlated-topic-model" class="nav-link" data-scroll-target="#correlated-topic-model">Correlated Topic Model</a></li>
  <li><a href="#structural-topic-model-1" id="toc-structural-topic-model-1" class="nav-link" data-scroll-target="#structural-topic-model-1">Structural Topic model</a></li>
  <li><a href="#estimate-effect" id="toc-estimate-effect" class="nav-link" data-scroll-target="#estimate-effect">Estimate Effect</a></li>
  <li><a href="#choosing-k" id="toc-choosing-k" class="nav-link" data-scroll-target="#choosing-k">Choosing K</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Tutorial10_Topic Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this tutorial we’ll learn about <strong>K-Means</strong> and <strong>topic models</strong> of two different types, the regular vanilla LDA version, and structural topic models.</p>
<section id="k-means" class="level1">
<h1>K-Means</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. The objective of K-means is: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.</p>
<p>In this tutorial, we are going to cluster a dataset consisting of health news tweets. These short sentences belong to one of the 16 sources of news considered in the dataset. We are then facing a multi-label classifying problem, with k = 16.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>truth.K <span class="ot">&lt;-</span> <span class="dv">16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="front-end-matters" class="level2">
<h2 class="anchored" data-anchor-id="front-end-matters">Front-end Matters</h2>
<p>First, let’s looad the <code>tm</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: NLP</code></pre>
</div>
</div>
<p>We download the data from the UCI Machine Learning Repository.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creating the empty dataset with the formatted columns</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>dataframe <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">ID =</span> <span class="fu">character</span>(),</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">datetime =</span> <span class="fu">character</span>(),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">content =</span> <span class="fu">character</span>(),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">label =</span> <span class="fu">factor</span>())</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>source.url <span class="ot">&lt;-</span> <span class="st">'https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip'</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>target.directory <span class="ot">&lt;-</span> <span class="st">'/tmp/clustering-r'</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>temporary.file <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(source.url, temporary.file)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="fu">unzip</span>(temporary.file, <span class="at">exdir =</span> target.directory)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Reading the files</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>target.directory <span class="ot">&lt;-</span> <span class="fu">paste</span>(target.directory, <span class="st">'Health-Tweets'</span>, <span class="at">sep=</span><span class="st">"/"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>files <span class="ot">&lt;-</span> <span class="fu">list.files</span>(<span class="at">path =</span> target.directory, <span class="at">pattern =</span> <span class="st">'.txt$'</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># filling the dataframe by reading the text content</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (f <span class="cf">in</span> files){</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>  news.filename <span class="ot">=</span> <span class="fu">paste</span>(target.directory, f, <span class="at">sep =</span> <span class="st">"/"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>  news.label <span class="ot">&lt;-</span> <span class="fu">substr</span>(f, <span class="dv">0</span>, <span class="fu">nchar</span>(f) <span class="sc">-</span> <span class="dv">4</span>) <span class="co"># removing the 4 last characters (.txt)</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>  news.data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(news.filename,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                        <span class="at">encoding =</span> <span class="st">"UTF-8"</span>,</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                        <span class="at">header =</span> <span class="cn">FALSE</span>,</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                        <span class="at">quote =</span> <span class="st">""</span>,</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                        <span class="at">sep =</span> <span class="st">"|"</span>,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>                        <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"ID"</span>, <span class="st">"datetime"</span>, <span class="st">"content"</span>))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Trick to ignore last part of tweets which content contains the split character "|"</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># no satisfying solution has been found to split and merging extra-columns with the last one</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>  news.data <span class="ot">&lt;-</span> news.data[news.data<span class="sc">$</span>content <span class="sc">!=</span> <span class="st">""</span>, ]</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>  news.data[<span class="st">'label'</span>] <span class="ot">=</span> news.label <span class="co"># we add the label of the tweet</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># only considering a little portion of data</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># because handling sparse matrix for generic usage is a pain</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>  news.data <span class="ot">&lt;-</span> <span class="fu">head</span>(news.data, <span class="fu">floor</span>(<span class="fu">nrow</span>(news.data) <span class="sc">*</span> <span class="fl">0.05</span>))</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>  dataframe <span class="ot">&lt;-</span> <span class="fu">rbind</span>(dataframe, news.data)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co"># deleting the temporary directory</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="fu">unlink</span>(target.directory, <span class="at">recursive =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<p>Removing urls in the tweets</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>sentences <span class="ot">&lt;-</span> <span class="fu">sub</span>(<span class="st">"http://([[:alnum:]|[:punct:]])+"</span>, <span class="st">''</span>, dataframe<span class="sc">$</span>content)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(sentences)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Breast cancer risk test devised "     
[2] "GP workload harming care - BMA poll " 
[3] "Short people's 'heart risk greater' " 
[4] "New approach against HIV 'promising' "
[5] "Coalition 'undermined NHS' - doctors "
[6] "Review of case against NHS manager "  </code></pre>
</div>
</div>
<p>For common preprocessing problems, we are going to use <code>tm</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>corpus <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">Corpus</span>(tm<span class="sc">::</span><span class="fu">VectorSource</span>(sentences))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># cleaning up</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># handling utf-8 encoding problem from the dataset</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>corpus.cleaned <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">tm_map</span>(corpus, <span class="cf">function</span>(x) <span class="fu">iconv</span>(x, <span class="at">to =</span> <span class="st">'UTF-8-MAC'</span>, <span class="at">sub =</span> <span class="st">'byte'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in tm_map.SimpleCorpus(corpus, function(x) iconv(x, to = "UTF-8-MAC", :
transformation drops documents</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>corpus.cleaned <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">tm_map</span>(corpus.cleaned, tm<span class="sc">::</span>removeWords, tm<span class="sc">::</span><span class="fu">stopwords</span>(<span class="st">'english'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in tm_map.SimpleCorpus(corpus.cleaned, tm::removeWords,
tm::stopwords("english")): transformation drops documents</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>corpus.cleaned <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">tm_map</span>(corpus.cleaned, tm<span class="sc">::</span>stripWhitespace)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in tm_map.SimpleCorpus(corpus.cleaned, tm::stripWhitespace):
transformation drops documents</code></pre>
</div>
</div>
</section>
<section id="text-representation" class="level2">
<h2 class="anchored" data-anchor-id="text-representation">Text Representation</h2>
<p>Now, we have a sequence of cleaned sentences that we can use to build our <strong>TF-IDF matrix</strong>. From this result, we will be able to execute every numerical processes that we want, such as <strong>clustering</strong>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Building the feature matrices</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>tfm <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">DocumentTermMatrix</span>(corpus.cleaned)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tfm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3159 9410</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tfm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;&lt;DocumentTermMatrix (documents: 3159, terms: 9410)&gt;&gt;
Non-/sparse entries: 26424/29699766
Sparsity           : 100%
Maximal term length: 62
Weighting          : term frequency (tf)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>tfm.tfidf <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">weightTfIdf</span>(tfm)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tfm.tfidf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3159 9410</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>tfm.tfidf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;&lt;DocumentTermMatrix (documents: 3159, terms: 9410)&gt;&gt;
Non-/sparse entries: 26424/29699766
Sparsity           : 100%
Maximal term length: 62
Weighting          : term frequency - inverse document frequency (normalized) (tf-idf)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we remove a lot of features. </span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>tfm.tfidf <span class="ot">&lt;-</span> tm<span class="sc">::</span><span class="fu">removeSparseTerms</span>(tfm.tfidf, <span class="fl">0.999</span>) <span class="co"># (data,allowed sparsity)</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>tfidf.matrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(tfm.tfidf)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tfidf.matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3159 1327</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cosine distance matrix (useful for specific clustering algorithms)</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>dist.matrix <span class="ot">=</span> proxy<span class="sc">::</span><span class="fu">dist</span>(tfidf.matrix, <span class="at">method =</span> <span class="st">"cosine"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="running-the-clustering-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="running-the-clustering-algorithms">Running the clustering algorithms</h2>
<section id="k-means-1" class="level3">
<h3 class="anchored" data-anchor-id="k-means-1">K-means</h3>
<p>Define clusters so that the total within-cluster variation is minimized.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Hartigan-Wong algorithm (Hartigan and Wong 1979) defines the total within-cluster variation as the sum of squared Euclidean distances between items and the corresponding centroid:</p>
<p><span class="math inline">\(W(C_{k}) = \sum_{x_{i} \in C_{k}}(x_{i} - \mu_{k})^{2}\)</span></p>
<ul>
<li><span class="math inline">\(x_{i}\)</span>: a data point belonging to the cluster <span class="math inline">\(C_{k}\)</span></li>
<li><span class="math inline">\(\mu_{k}\)</span>: the mean value of the points assigned to the cluster <span class="math inline">\(C_{k}\)</span></li>
</ul>
<p>Total within-cluster variation as follows:</p>
<p>total withinness = <span class="math inline">\(\sum^{k}_{k=1}W(C_{k}) = \sum^{k}_{k=1} \sum_{x_{i} \in C_{k}} (x_{i} - \mu_{k})^{2}\)</span></p>
<p>The total within-cluster sum of square measures the goodness of the clustering and we want it to be as small as possible.</p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>clustering.kmeans <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(tfidf.matrix, truth.K)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(clustering.kmeans)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      </code></pre>
</div>
</div>
</section>
<section id="hierarchical-clustering" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-clustering">Hierarchical clustering</h3>
<p>Define a clustering criterion and the pointwise distance matrix. Let’s use the Ward’s methods as the clustering criterion.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>clustering.hierarchical <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist.matrix, <span class="at">method =</span> <span class="st">"ward.D2"</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(clustering.hierarchical)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "merge"       "height"      "order"       "labels"      "method"     
[6] "call"        "dist.method"</code></pre>
</div>
</div>
</section>
<section id="plotting" class="level3">
<h3 class="anchored" data-anchor-id="plotting">Plotting</h3>
<p>To plot the clustering results, as our feature spaces is highly dimensional (TF-IDF representation), we will reduce it to 2 thanks to multi-dimensional scaling. This technique is dependent of our distance metric, but in our case with TF-IDF.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>points <span class="ot">&lt;-</span> <span class="fu">cmdscale</span>(dist.matrix, <span class="at">k =</span> <span class="dv">2</span>) <span class="co"># running the PCA </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>palette <span class="ot">&lt;-</span> colorspace<span class="sc">::</span><span class="fu">diverge_hcl</span>(truth.K) <span class="co"># creating a color palette</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>previous.par <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))<span class="co"># partitioning the plot space</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>master.cluster <span class="ot">&lt;-</span> clustering.kmeans<span class="sc">$</span>cluster</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(points,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">'K-Means clustering'</span>,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">as.factor</span>(master.cluster),</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">mai =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">'n'</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>,</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">''</span>, <span class="at">ylab =</span> <span class="st">''</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>slave.hierarchical <span class="ot">&lt;-</span> <span class="fu">cutree</span>(clustering.hierarchical, <span class="at">k =</span> truth.K)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(points,</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">'Hierarchical clustering'</span>,</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">as.factor</span>(slave.hierarchical),</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">mai =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">'n'</span>, <span class="at">yaxt =</span> <span class="st">'n'</span>,</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">''</span>, <span class="at">ylab =</span> <span class="st">''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Tutorial10_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(previous.par) <span class="co"># recovering the original plot space parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="determining-k" class="level2">
<h2 class="anchored" data-anchor-id="determining-k">Determining K</h2>
<p>In the previous example, we know sentences belong to one of the 16 sources. Then how to decide the best number of clusters (K)?</p>
<p>Here we use the “eblow” method. For each given number of clusters, we can calculate how much variance in the data can be explained by the clustering. Typically, this will increase with the number of clusters. However, the increase would slow down at a certain point and that’s where we choose the number of clusters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">16</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>varper <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  clustering.kmeans2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(tfidf.matrix, i)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  varper <span class="ot">&lt;-</span> <span class="fu">c</span>(varper, <span class="fu">as.numeric</span>(clustering.kmeans2<span class="sc">$</span>betweenss)<span class="sc">/</span><span class="fu">as.numeric</span>(clustering.kmeans2<span class="sc">$</span>totss))</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>varper</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 3.842354e-12 5.560551e-03 1.916852e-02 2.421237e-02 2.373344e-02
 [6] 2.982853e-02 2.815610e-02 3.071530e-02 3.742710e-02 3.557937e-02
[11] 4.460016e-02 4.271404e-02 4.308681e-02 4.716389e-02 4.749830e-02
[16] 5.124432e-02</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>k, varper, <span class="at">xlab =</span> <span class="st">"# of clusters"</span>, <span class="at">ylab =</span> <span class="st">"explained variance"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Tutorial10_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>From the plot, after 3 clusters, the increase in the explained variance becomes slower - there is an elbow here. Therefore, we might use 3 clusters here.</p>
</section>
</section>
<section id="topic-models" class="level1">
<h1>Topic Models</h1>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">Introduction</h2>
<p>The general idea with topic models is to identify the topics that characterize a set of documents. The background on this is interesting; a lot of the initial interest came from digital humanities and library science where you had the need to systematically organize the massive thematic content of the huge collections of texts. Importantly, LDA and STM, the two we’ll discuss this week, are both <strong>mixed-membership</strong> models, meaning documents are characterized as arising from a distribution over topics, rather than coming from a single topic.</p>
</section>
<section id="latent-dirichlet-allocation" class="level2">
<h2 class="anchored" data-anchor-id="latent-dirichlet-allocation">Latent Dirichlet Allocation</h2>
<p>For LDA, we will be using the <code>text2vec</code> package. It is an <code>R</code> package that provides an efficient framework for text analysis and NLP. It’s a fast implementation of word embedding models (which is where it gets it’s name from) but it also has really nice and <strong>fast</strong> functionality for LDA.</p>
<p>Algorithms may classify topics within a text set, and <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation (LDA)</a> is one of the most popular algorithms for topic modeling. LDA uses two basic principles:</p>
<ol type="1">
<li>Each document is made up of topics.</li>
<li>Each word in a document can be attributed to a topic.</li>
</ol>
<p>Let’s begin!</p>
<section id="front-end-matter" class="level3">
<h3 class="anchored" data-anchor-id="front-end-matter">Front-end Matter</h3>
<p>First, let’s load the <code>text2vec</code> package:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(text2vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>We will be using the built in movie reviews dataset that comes with the package.</strong> It is labeled and can be called as “movie_review”. Let’s load it in:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load in built-in dataset</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"movie_review"</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Prints first ten rows of the dtaset:</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(movie_review, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        id sentiment
1   5814_8         1
2   2381_9         1
3   7759_3         0
4   3630_4         0
5   9495_8         1
6   8196_8         1
7   7166_2         0
8  10633_1         0
9    319_1         0
10 8713_10         1
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        review
1                                                                                                                                               With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.&lt;br /&gt;&lt;br /&gt;Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.&lt;br /&gt;&lt;br /&gt;The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.&lt;br /&gt;&lt;br /&gt;Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.&lt;br /&gt;&lt;br /&gt;Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.
2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\"The Classic War of the Worlds\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\"critics\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\"critics\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\"critics\\" perceive to be its shortcomings.
3  The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like Jurassik Park, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.&lt;br /&gt;&lt;br /&gt;The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : Sabretooth(2002)by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better 10.000 BC(2006) by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.
4                                                                                                                                                                                                  It must be assumed that those who praised this film (\\"the greatest filmed opera ever,\\" didn't I read somewhere?) either don't care for opera, don't care for Wagner, or don't care about anything except their desire to appear Cultured. Either as a representation of Wagner's swan-song, or as a movie, this strikes me as an unmitigated disaster, with a leaden reading of the score matched to a tricksy, lugubrious realisation of the text.&lt;br /&gt;&lt;br /&gt;It's questionable that people with ideas as to what an opera (or, for that matter, a play, especially one by Shakespeare) is \\"about\\" should be allowed anywhere near a theatre or film studio; Syberberg, very fashionably, but without the smallest justification from Wagner's text, decided that Parsifal is \\"about\\" bisexual integration, so that the title character, in the latter stages, transmutes into a kind of beatnik babe, though one who continues to sing high tenor -- few if any of the actors in the film are the singers, and we get a double dose of Armin Jordan, the conductor, who is seen as the face (but not heard as the voice) of Amfortas, and also appears monstrously in double exposure as a kind of Batonzilla or Conductor Who Ate Monsalvat during the playing of the Good Friday music -- in which, by the way, the transcendant loveliness of nature is represented by a scattering of shopworn and flaccid crocuses stuck in ill-laid turf, an expedient which baffles me. In the theatre we sometimes have to piece out such imperfections with our thoughts, but I can't think why Syberberg couldn't splice in, for Parsifal and Gurnemanz, mountain pasture as lush as was provided for Julie Andrews in Sound of Music...&lt;br /&gt;&lt;br /&gt;The sound is hard to endure, the high voices and the trumpets in particular possessing an aural glare that adds another sort of fatigue to our impatience with the uninspired conducting and paralytic unfolding of the ritual. Someone in another review mentioned the 1951 Bayreuth recording, and Knappertsbusch, though his tempi are often very slow, had what Jordan altogether lacks, a sense of pulse, a feeling for the ebb and flow of the music -- and, after half a century, the orchestral sound in that set, in modern pressings, is still superior to this film.
5                                                                                                                                                                                                                  Superbly trashy and wondrously unpretentious 80's exploitation, hooray! The pre-credits opening sequences somewhat give the false impression that we're dealing with a serious and harrowing drama, but you need not fear because barely ten minutes later we're up until our necks in nonsensical chainsaw battles, rough fist-fights, lurid dialogs and gratuitous nudity! Bo and Ingrid are two orphaned siblings with an unusually close and even slightly perverted relationship. Can you imagine playfully ripping off the towel that covers your sister's naked body and then stare at her unshaven genitals for several whole minutes? Well, Bo does that to his sister and, judging by her dubbed laughter, she doesn't mind at all. Sick, dude! Anyway, as kids they fled from Russia with their parents, but nasty soldiers brutally slaughtered mommy and daddy. A friendly smuggler took custody over them, however, and even raised and trained Bo and Ingrid into expert smugglers. When the actual plot lifts off, 20 years later, they're facing their ultimate quest as the mythical and incredibly valuable White Fire diamond is coincidentally found in a mine. Very few things in life ever made as little sense as the plot and narrative structure of \\"White Fire\\", but it sure is a lot of fun to watch. Most of the time you have no clue who's beating up who or for what cause (and I bet the actors understood even less) but whatever! The violence is magnificently grotesque and every single plot twist is pleasingly retarded. The script goes totally bonkers beyond repair when suddenly  and I won't reveal for what reason  Bo needs a replacement for Ingrid and Fred Williamson enters the scene with a big cigar in his mouth and his sleazy black fingers all over the local prostitutes. Bo's principal opponent is an Italian chick with big breasts but a hideous accent, the preposterous but catchy theme song plays at least a dozen times throughout the film, there's the obligatory \\"we're-falling-in-love\\" montage and loads of other attractions! My God, what a brilliant experience. The original French title translates itself as \\"Life to Survive\\", which is uniquely appropriate because it makes just as much sense as the rest of the movie: None!
6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I dont know why people think this is such a bad movie. Its got a pretty good plot, some good action, and the change of location for Harry does not hurt either. Sure some of its offensive and gratuitous but this is not the only movie like that. Eastwood is in good form as Dirty Harry, and I liked Pat Hingle in this movie as the small town cop. If you liked DIRTY HARRY, then you should see this one, its a lot better than THE DEAD POOL. 4/5
7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                This movie could have been very good, but comes up way short. Cheesy special effects and so-so acting. I could have looked past that if the story wasn't so lousy. If there was more of a background story, it would have been better. The plot centers around an evil Druid witch who is linked to this woman who gets migraines. The movie drags on and on and never clearly explains anything, it just keeps plodding on. Christopher Walken has a part, but it is completely senseless, as is most of the movie. This movie had potential, but it looks like some really bad made for TV movie. I would avoid this movie.
8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I watched this video at a friend's house. I'm glad I did not waste money buying this one. The video cover has a scene from the 1975 movie Capricorn One. The movie starts out with several clips of rocket blow-ups, most not related to manned flight. Sibrel's smoking gun is a short video clip of the astronauts preparing a video broadcast. He edits in his own voice-over instead of letting us listen to what the crew had to say. The video curiously ends with a showing of the Zapruder film. His claims about radiation, shielding, star photography, and others lead me to believe is he extremely ignorant or has some sort of ax to grind against NASA, the astronauts, or American in general. His science is bad, and so is this video.
9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A friend of mine bought this film for 1, and even then it was grossly overpriced. Despite featuring big names such as Adam Sandler, Billy Bob Thornton and the incredibly talented Burt Young, this film was about as funny as taking a chisel and hammering it straight through your earhole. It uses tired, bottom of the barrel comedic techniques - consistently breaking the fourth wall as Sandler talks to the audience, and seemingly pointless montages of 'hot girls'.&lt;br /&gt;&lt;br /&gt;Adam Sandler plays a waiter on a cruise ship who wants to make it as a successful comedian in order to become successful with women. When the ship's resident comedian - the shamelessly named 'Dickie' due to his unfathomable success with the opposite gender - is presumed lost at sea, Sandler's character Shecker gets his big break. Dickie is not dead, he's rather locked in the bathroom, presumably sea sick.&lt;br /&gt;&lt;br /&gt;Perhaps from his mouth he just vomited the worst film of all time.
10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         &lt;br /&gt;&lt;br /&gt;This movie is full of references. Like \\"Mad Max II\\", \\"The wild one\\" and many others. The ladybugs face its a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. Well talk much more about in the future.</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># checking dimensions of dataset</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(movie_review)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5000    3</code></pre>
</div>
</div>
<p>The dataset consists of 5000 movie reviews, each of which is marked as positive (1) or negative (0) in the ‘sentiment’ column.</p>
<p>Now, we need to clean the data up a bit. To make our lives easier and limit the amount of processing power, let’s use the first 3000 reviews. They are located in the ‘review’ column.</p>
</section>
<section id="vectorization" class="level3">
<h3 class="anchored" data-anchor-id="vectorization">Vectorization</h3>
<p>Texts can take up a lot of memory themselves, but vectorized texts typically do not. To represent documents in vector space, we first have to come to create mappings from terms to term IDs. We call them terms instead of words because they can be arbitrary n-grams not just single words. We represent a set of documents as a sparse matrix, where each row corresponds to a document and each column corresponds to a term. This can be done in two ways: using the vocabulary itself or by <a href="https://en.wikipedia.org/wiki/Feature_hashing">feature hashing</a>.</p>
<ul>
<li><a href="https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html#vectorization">Information gathered from <code>text2vec</code> creator.</a>.</li>
</ul>
<p>Let’s perform tokenization and lowercase each token:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creates string of combined lowercased words</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="ot">&lt;-</span> <span class="fu">tolower</span>(movie_review<span class="sc">$</span>review[<span class="dv">1</span><span class="sc">:</span><span class="dv">3000</span>])</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># performs tokenization</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>tokens <span class="ot">&lt;-</span> <span class="fu">word_tokenizer</span>(tokens)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># prints first two tokenized rows</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tokens, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
  [1] "with"         "all"          "this"         "stuff"        "going"       
  [6] "down"         "at"           "the"          "moment"       "with"        
 [11] "mj"           "i've"         "started"      "listening"    "to"          
 [16] "his"          "music"        "watching"     "the"          "odd"         
 [21] "documentary"  "here"         "and"          "there"        "watched"     
 [26] "the"          "wiz"          "and"          "watched"      "moonwalker"  
 [31] "again"        "maybe"        "i"            "just"         "want"        
 [36] "to"           "get"          "a"            "certain"      "insight"     
 [41] "into"         "this"         "guy"          "who"          "i"           
 [46] "thought"      "was"          "really"       "cool"         "in"          
 [51] "the"          "eighties"     "just"         "to"           "maybe"       
 [56] "make"         "up"           "my"           "mind"         "whether"     
 [61] "he"           "is"           "guilty"       "or"           "innocent"    
 [66] "moonwalker"   "is"           "part"         "biography"    "part"        
 [71] "feature"      "film"         "which"        "i"            "remember"    
 [76] "going"        "to"           "see"          "at"           "the"         
 [81] "cinema"       "when"         "it"           "was"          "originally"  
 [86] "released"     "some"         "of"           "it"           "has"         
 [91] "subtle"       "messages"     "about"        "mj's"         "feeling"     
 [96] "towards"      "the"          "press"        "and"          "also"        
[101] "the"          "obvious"      "message"      "of"           "drugs"       
[106] "are"          "bad"          "m'kay"        "br"           "br"          
[111] "visually"     "impressive"   "but"          "of"           "course"      
[116] "this"         "is"           "all"          "about"        "michael"     
[121] "jackson"      "so"           "unless"       "you"          "remotely"    
[126] "like"         "mj"           "in"           "anyway"       "then"        
[131] "you"          "are"          "going"        "to"           "hate"        
[136] "this"         "and"          "find"         "it"           "boring"      
[141] "some"         "may"          "call"         "mj"           "an"          
[146] "egotist"      "for"          "consenting"   "to"           "the"         
[151] "making"       "of"           "this"         "movie"        "but"         
[156] "mj"           "and"          "most"         "of"           "his"         
[161] "fans"         "would"        "say"          "that"         "he"          
[166] "made"         "it"           "for"          "the"          "fans"        
[171] "which"        "if"           "true"         "is"           "really"      
[176] "nice"         "of"           "him"          "br"           "br"          
[181] "the"          "actual"       "feature"      "film"         "bit"         
[186] "when"         "it"           "finally"      "starts"       "is"          
[191] "only"         "on"           "for"          "20"           "minutes"     
[196] "or"           "so"           "excluding"    "the"          "smooth"      
[201] "criminal"     "sequence"     "and"          "joe"          "pesci"       
[206] "is"           "convincing"   "as"           "a"            "psychopathic"
[211] "all"          "powerful"     "drug"         "lord"         "why"         
[216] "he"           "wants"        "mj"           "dead"         "so"          
[221] "bad"          "is"           "beyond"       "me"           "because"     
[226] "mj"           "overheard"    "his"          "plans"        "nah"         
[231] "joe"          "pesci's"      "character"    "ranted"       "that"        
[236] "he"           "wanted"       "people"       "to"           "know"        
[241] "it"           "is"           "he"           "who"          "is"          
[246] "supplying"    "drugs"        "etc"          "so"           "i"           
[251] "dunno"        "maybe"        "he"           "just"         "hates"       
[256] "mj's"         "music"        "br"           "br"           "lots"        
[261] "of"           "cool"         "things"       "in"           "this"        
[266] "like"         "mj"           "turning"      "into"         "a"           
[271] "car"          "and"          "a"            "robot"        "and"         
[276] "the"          "whole"        "speed"        "demon"        "sequence"    
[281] "also"         "the"          "director"     "must"         "have"        
[286] "had"          "the"          "patience"     "of"           "a"           
[291] "saint"        "when"         "it"           "came"         "to"          
[296] "filming"      "the"          "kiddy"        "bad"          "sequence"    
[301] "as"           "usually"      "directors"    "hate"         "working"     
[306] "with"         "one"          "kid"          "let"          "alone"       
[311] "a"            "whole"        "bunch"        "of"           "them"        
[316] "performing"   "a"            "complex"      "dance"        "scene"       
[321] "br"           "br"           "bottom"       "line"         "this"        
[326] "movie"        "is"           "for"          "people"       "who"         
[331] "like"         "mj"           "on"           "one"          "level"       
[336] "or"           "another"      "which"        "i"            "think"       
[341] "is"           "most"         "people"       "if"           "not"         
[346] "then"         "stay"         "away"         "it"           "does"        
[351] "try"          "and"          "give"         "off"          "a"           
[356] "wholesome"    "message"      "and"          "ironically"   "mj's"        
[361] "bestest"      "buddy"        "in"           "this"         "movie"       
[366] "is"           "a"            "girl"         "michael"      "jackson"     
[371] "is"           "truly"        "one"          "of"           "the"         
[376] "most"         "talented"     "people"       "ever"         "to"          
[381] "grace"        "this"         "planet"       "but"          "is"          
[386] "he"           "guilty"       "well"         "with"         "all"         
[391] "the"          "attention"    "i've"         "gave"         "this"        
[396] "subject"      "hmmm"         "well"         "i"            "don't"       
[401] "know"         "because"      "people"       "can"          "be"          
[406] "different"    "behind"       "closed"       "doors"        "i"           
[411] "know"         "this"         "for"          "a"            "fact"        
[416] "he"           "is"           "either"       "an"           "extremely"   
[421] "nice"         "but"          "stupid"       "guy"          "or"          
[426] "one"          "of"           "the"          "most"         "sickest"     
[431] "liars"        "i"            "hope"         "he"           "is"          
[436] "not"          "the"          "latter"      

[[2]]
  [1] "the"          "classic"      "war"          "of"           "the"         
  [6] "worlds"       "by"           "timothy"      "hines"        "is"          
 [11] "a"            "very"         "entertaining" "film"         "that"        
 [16] "obviously"    "goes"         "to"           "great"        "effort"      
 [21] "and"          "lengths"      "to"           "faithfully"   "recreate"    
 [26] "h"            "g"            "wells"        "classic"      "book"        
 [31] "mr"           "hines"        "succeeds"     "in"           "doing"       
 [36] "so"           "i"            "and"          "those"        "who"         
 [41] "watched"      "his"          "film"         "with"         "me"          
 [46] "appreciated"  "the"          "fact"         "that"         "it"          
 [51] "was"          "not"          "the"          "standard"     "predictable" 
 [56] "hollywood"    "fare"         "that"         "comes"        "out"         
 [61] "every"        "year"         "e.g"          "the"          "spielberg"   
 [66] "version"      "with"         "tom"          "cruise"       "that"        
 [71] "had"          "only"         "the"          "slightest"    "resemblance" 
 [76] "to"           "the"          "book"         "obviously"    "everyone"    
 [81] "looks"        "for"          "different"    "things"       "in"          
 [86] "a"            "movie"        "those"        "who"          "envision"    
 [91] "themselves"   "as"           "amateur"      "critics"      "look"        
 [96] "only"         "to"           "criticize"    "everything"   "they"        
[101] "can"          "others"       "rate"         "a"            "movie"       
[106] "on"           "more"         "important"    "bases"        "like"        
[111] "being"        "entertained"  "which"        "is"           "why"         
[116] "most"         "people"       "never"        "agree"        "with"        
[121] "the"          "critics"      "we"           "enjoyed"      "the"         
[126] "effort"       "mr"           "hines"        "put"          "into"        
[131] "being"        "faithful"     "to"           "h.g"          "wells"       
[136] "classic"      "novel"        "and"          "we"           "found"       
[141] "it"           "to"           "be"           "very"         "entertaining"
[146] "this"         "made"         "it"           "easy"         "to"          
[151] "overlook"     "what"         "the"          "critics"      "perceive"    
[156] "to"           "be"           "its"          "shortcomings"</code></pre>
</div>
</div>
<p>Note that <code>text2vec</code> provides a few tokenizer functions (see <code>?tokenizers)</code>. These are just simple wrappers for the <code>base::gsub()</code> function and are not very fast or flexible. If you need something smarter or faster you can use the <code>tokenizers</code> package.</p>
<p>We can create an iterator over each token using <code>itoken()</code>. An iterator is an object that can be iterated upon, meaning that you can traverse through all the values. In our example, we’ll be able to traverse through each token for each row using our newly generated iterator, <code>it</code>. The general thing to note here is that this is a way to make the approach less memory intensive, something that will turn out to be helpful.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># iterates over each token</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>it <span class="ot">&lt;-</span> <span class="fu">itoken</span>(tokens, <span class="at">ids =</span> movie_review<span class="sc">$</span>id[<span class="dv">1</span><span class="sc">:</span><span class="dv">3000</span>], <span class="at">progressbar =</span> <span class="cn">FALSE</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># prints iterator</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>it</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;itoken&gt;
  Inherits from: &lt;CallbackIterator&gt;
  Public:
    callback: function (x) 
    clone: function (deep = FALSE) 
    initialize: function (x, callback = identity) 
    is_complete: active binding
    length: active binding
    move_cursor: function () 
    nextElem: function () 
    x: GenericIterator, iterator, R6</code></pre>
</div>
</div>
</section>
<section id="vocabulary-based-vectorization" class="level3">
<h3 class="anchored" data-anchor-id="vocabulary-based-vectorization">Vocabulary-based Vectorization</h3>
<p>As stated above, we represent our corpus as a document-feature matrix. The process for <code>text2vec</code> is much different than with <code>quanteda</code>, though the intuition is generally aligned. Effectively, the <code>text2vec</code> design is intended to be faster and more memory-efficient; the downside is that it’s a little more obtuse. The first step is to create our vocabulary for the DFM. That is simple since we have already created an iterator; all we need to do is place our iterator as an argument inside <code>create_vocabulary()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># built the vocabulary</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">create_vocabulary</span>(it)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print vocabulary</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>v</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of docs: 3000 
0 stopwords:  ... 
ngram_min = 1; ngram_max = 1 
Vocabulary: 
        term term_count doc_count
    1:   0.3          1         1
    2:  0.48          1         1
    3:   0.5          1         1
    4:  0.89          1         1
    5: 00015          1         1
   ---                           
33487:    to      16370      2826
33488:    of      17409      2829
33489:   and      19761      2892
33490:     a      19776      2910
33491:   the      40246      2975</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># checking dimensions</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 33491     3</code></pre>
</div>
</div>
<p>We can create stop words or prune our vocabulary with <code>prune_vocabulary()</code>. We will keep the terms that occur at least 10 times.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prunes vocabulary</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">prune_vocabulary</span>(v, <span class="at">term_count_min =</span> <span class="dv">10</span>, <span class="at">doc_proportion_max =</span> <span class="fl">0.2</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># check dimensions</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5325    3</code></pre>
</div>
</div>
<p>If we check the dimensions after pruning our vocabulary, we can see that we have less terms. We have removed the very common words so that our vocabulary can contain more high quality and meaningful words.</p>
<p>Before we can create our DFM, we’ll need to vectorize our vocabulary with <code>vocab_vectorizer()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creates a closure that helps transform list of tokens into vector space</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="ot">&lt;-</span> <span class="fu">vocab_vectorizer</span>(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now have everything we need to create a DFM. We can pass in our iterator of tokens, our vectorized vocabulary, and a type of matrix (either <code>dgCMatrix</code> or <code>dgTMatrix</code>) in <code>create_dtm()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creates document term matrix</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> <span class="fu">create_dtm</span>(it, vectorizer, <span class="at">type =</span> <span class="st">"dgTMatrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can create our topic model after we have created our DTM. We create our model using <code>LDA$new()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create new LDA model</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>lda_model <span class="ot">&lt;-</span> LDA<span class="sc">$</span><span class="fu">new</span>(<span class="at">n_topics =</span> <span class="dv">10</span>, <span class="at">doc_topic_prior =</span> <span class="fl">0.1</span>,</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">topic_word_prior =</span> <span class="fl">0.01</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co"># print other methods for LDA</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>lda_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;WarpLDA&gt;
  Inherits from: &lt;LDA&gt;
  Public:
    clone: function (deep = FALSE) 
    components: active binding
    fit_transform: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, 
    get_top_words: function (n = 10, topic_number = 1L:private$n_topics, lambda = 1) 
    initialize: function (n_topics = 10L, doc_topic_prior = 50/n_topics, topic_word_prior = 1/n_topics, 
    plot: function (lambda.step = 0.1, reorder.topics = FALSE, doc_len = private$doc_len, 
    topic_word_distribution: active binding
    transform: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, 
  Private:
    calc_pseudo_loglikelihood: function (ptr = private$ptr) 
    check_convert_input: function (x) 
    components_: NULL
    doc_len: NULL
    doc_topic_distribution: function () 
    doc_topic_distribution_with_prior: function () 
    doc_topic_matrix: NULL
    doc_topic_prior: 0.1
    fit_transform_internal: function (model_ptr, n_iter, convergence_tol, n_check_convergence, 
    get_c_all: function () 
    get_c_all_local: function () 
    get_doc_topic_matrix: function (prt, nr) 
    get_topic_word_count: function () 
    init_model_dtm: function (x, ptr = private$ptr) 
    internal_matrix_formats: list
    is_initialized: FALSE
    n_iter_inference: 10
    n_topics: 10
    ptr: NULL
    reset_c_local: function () 
    run_iter_doc: function (update_topics = TRUE, ptr = private$ptr) 
    run_iter_word: function (update_topics = TRUE, ptr = private$ptr) 
    seeds: 154317648.356281 1121893281.95516
    set_c_all: function (x) 
    set_internal_matrix_formats: function (sparse = NULL, dense = NULL) 
    topic_word_distribution_with_prior: function () 
    topic_word_prior: 0.01
    transform_internal: function (x, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, 
    vocabulary: NULL</code></pre>
</div>
</div>
<p>After printing <code>lda_model</code>, we can see there are other methods we can use with the model.</p>
<p>Note: the only accessible methods are the ones under ‘Public’. Documentation for all methods and arguments are available <a href="https://cran.r-project.org/web/packages/text2vec/text2vec.pdf">here</a> on page 22.</p>
</section>
<section id="fitting" class="level3">
<h3 class="anchored" data-anchor-id="fitting">Fitting</h3>
<p>We can fit our model with <code>$fit_transform</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fitting model</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>doc_topic_distr <span class="ot">&lt;-</span> </span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  lda_model<span class="sc">$</span><span class="fu">fit_transform</span>(<span class="at">x =</span> dtm, <span class="at">n_iter =</span> <span class="dv">1000</span>,</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">convergence_tol =</span> <span class="fl">0.001</span>, <span class="at">n_check_convergence =</span> <span class="dv">25</span>,</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">progressbar =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INFO  [11:52:41.420] early stopping at 225 iteration
INFO  [11:52:45.972] early stopping at 50 iteration</code></pre>
</div>
</div>
<p>The <code>doc_topic_distr</code> object is a matrix where each row is a document, each column is a topic, and the cell entry is the proportion of the document estimated to be of the topic. That is, each row is the topic attention distribution for a document.</p>
<p>For example, here’s the topic distribution for the very first document:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(doc_topic_distr[<span class="dv">1</span>, ], <span class="at">xlab =</span> <span class="st">"topic"</span>,</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab =</span> <span class="st">"proportion"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">names.arg =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(doc_topic_distr))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Tutorial10_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="describing-topics-top-words" class="level3">
<h3 class="anchored" data-anchor-id="describing-topics-top-words">Describing Topics: Top Words</h3>
<p>We can also use <code>$get_top_words</code> as a method to get the top words for each topic.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get top n words for topics 1, 5, and 10</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span><span class="fu">get_top_words</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">topic_number =</span> <span class="fu">c</span>(1L, 5L, 10L),</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">lambda =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]     [,2]          [,3]    
 [1,] "ever"   "performance" "now"   
 [2,] "i'm"    "scenes"      "game"  
 [3,] "didn't" "director"    "old"   
 [4,] "know"   "makes"       "still" 
 [5,] "worst"  "may"         "joe"   
 [6,] "say"    "new"         "he's"  
 [7,] "your"   "these"       "man"   
 [8,] "got"    "here"        "played"
 [9,] "funny"  "both"        "name"  
[10,] "thing"  "main"        "star"  </code></pre>
</div>
</div>
<p>Also top-words could be stored by “relevance” which also takes into account frequency of word in the corpus (0 &lt; lambda &lt; 1).</p>
<p>The creator recommends setting lambda to be between 0.2 and 0.4. Here’s the difference compared to a lambda of 1:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span><span class="fu">get_top_words</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">topic_number =</span> <span class="fu">c</span>(1L, 5L, 10L),</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">lambda =</span> <span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1]        [,2]        [,3]       
 [1,] "awful"     "thriller"  "joe"      
 [2,] "stupid"    "brings"    "game"     
 [3,] "cheesy"    "che"       "cowboy"   
 [4,] "waste"     "match"     "opera"    
 [5,] "worst"     "davis"     "johnny"   
 [6,] "porn"      "detective" "faces"    
 [7,] "budget"    "sullivan"  "don"      
 [8,] "screaming" "walter"    "invisible"
 [9,] "talking"   "perfectly" "race"     
[10,] "bomb"      "wwe"       "pilot"    </code></pre>
</div>
</div>
</section>
<section id="apply-learned-model-to-new-data" class="level3">
<h3 class="anchored" data-anchor-id="apply-learned-model-to-new-data">Apply Learned Model to New Data</h3>
<p>One thing we occasionally may be interested in doing is understanding how well our model fits the data. Therefore, we can rely on our supervised learning insights and apply the estimated model to new data. From that, we’ll obtain a document-topic distribution that we can:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creating iterator</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>it2 <span class="ot">&lt;-</span> <span class="fu">itoken</span>(movie_review<span class="sc">$</span>review[<span class="dv">3001</span><span class="sc">:</span><span class="dv">5000</span>], tolower,</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>              word_tokenizer, <span class="at">ids =</span> movie_review<span class="sc">$</span>id[<span class="dv">3001</span><span class="sc">:</span><span class="dv">5000</span>])</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># creating new DFM</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>new_dtm <span class="ot">&lt;-</span> <span class="fu">create_dtm</span>(it2, vectorizer, <span class="at">type =</span> <span class="st">"dgTMatrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will have to use <code>$transform</code> instead of <code>$fit_transform</code> since we don’t have to fit the new model (we are attempting to predict the last 2000).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>new_doc_topiic_distr <span class="ot">=</span> lda_model<span class="sc">$</span><span class="fu">transform</span>(new_dtm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INFO  [11:52:51.225] early stopping at 40 iteration</code></pre>
</div>
</div>
<p>One widely used approach for model hyper-parameter tuning is validation of per-word <em>perplexity</em> on hold-out set. This is quite easy with <code>text2vec</code>.</p>
<p>Remember that we’ve fit the model on only the first 3000 reviews and predicted the last 2000. Therefore, we will calculate the held-out perplexity on these 2000 docs as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculates perplexity between new and old topic word distribution</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="fu">perplexity</span>(new_dtm, <span class="at">topic_word_distribution =</span> lda_model<span class="sc">$</span>topic_word_distribution,</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">doc_topic_distribution =</span> new_doc_topiic_distr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2298.228</code></pre>
</div>
</div>
<p>The lower perplexity the better. We can imagine adapting our hyperparameters and re-estimating across perplexity to try to evaluate our model performance. Still, perplexity as a measure has it’s own concerns: it doesn’t directly provide insight on whether or not the topics make sense, and tends to prefer bigger models than smaller ones.</p>
</section>
<section id="visualization" class="level3">
<h3 class="anchored" data-anchor-id="visualization">Visualization</h3>
<p>Normally it would take one line to run the visualization for the LDA model, using the method <code>$plot()</code>.</p>
<p>Let’s download and load in the required library the visuals depend on:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages('LDAvis')</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(LDAvis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creating plot</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span><span class="fu">plot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required namespace: servr</code></pre>
</div>
</div>
</section>
</section>
<section id="structural-topic-model" class="level2">
<h2 class="anchored" data-anchor-id="structural-topic-model">Structural Topic Model</h2>
<p>Imagine you are interested in the topics that are explored in political speeches, and specifically whether Republicans and Democrats focus on different topics. One approach would be to–after estimating an LDA model like above–average the topic proportions by the speaker.</p>
<p>Of course, that seems inefficient. We might want to instead leverage the information on the speech itself <strong>as part of the estimation of the topics</strong>. That is, we are estimating topical prevalence, and we know that there’s a different speaker, so we should be incorporating that information in estimating the topics. That’s the fundamental idea with Structural Topic Models (STM).</p>
<section id="front-end-matters-1" class="level3">
<h3 class="anchored" data-anchor-id="front-end-matters-1">Front-end Matters</h3>
<p>STM has really fantastic documentation and a host of related packages for added functionality. You can find the STM website <a href="https://www.structuraltopicmodel.com/">here</a>. Let’s load the package. Note that this will almost certainly take a few minutes given all of the dependencies.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages("stm")</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>stm v1.3.6 successfully loaded. See ?stm for help. 
 Papers, resources, and other materials at structuraltopicmodel.com</code></pre>
</div>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Package version: 3.3.1
Unicode version: 14.0
ICU version: 70.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Parallel computing: 4 of 4 threads used.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>See https://quanteda.io for tutorials and examples.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'quanteda'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:tm':

    stopwords</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:NLP':

    meta, meta&lt;-</code></pre>
</div>
</div>
</section>
<section id="creating-the-dfm" class="level3">
<h3 class="anchored" data-anchor-id="creating-the-dfm">Creating the DFM</h3>
<p>We’ll continue to use the movie reviews dataset. Now, we’ll leverage the <code>sentiment</code> variable included in the dataset as a covariate in our estimates of topical prevalence; that is, we expect some topics to be more prevalent in positive reviews as opposed to negative reviews, and vice versa. The variable is coded [0,1], with 0 indicating a negative review and 1 indicating a positive review.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(movie_review<span class="sc">$</span>sentiment)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
   0    1 
2483 2517 </code></pre>
</div>
</div>
<p>STM works differently than the <code>text2vec</code>, so we’ll need to have our data in a different format now.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>myDfm <span class="ot">&lt;-</span> <span class="fu">dfm</span>(<span class="fu">tokens</span>(movie_review<span class="sc">$</span>review),</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">tolower =</span> <span class="cn">TRUE</span>,</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">remove =</span> <span class="fu">stopwords</span>(<span class="st">"en"</span>),</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">remove_punct =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: '...' should not be used for tokens() arguments; use 'tokens()' first.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: 'remove' is deprecated; use dfm_remove() instead</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(myDfm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  5000 47719</code></pre>
</div>
</div>
</section>
<section id="correlated-topic-model" class="level3">
<h3 class="anchored" data-anchor-id="correlated-topic-model">Correlated Topic Model</h3>
<p>Now that we have our corpus, we can prep for a structural topic model that incorporates covariates. Recall, however, that the STM <code>without covariates</code> reduces to a very fast implementation of Correlated Topic Models (i.e., a version of the vanilla LDA model but where the topic proportions can be positively correlated with one another).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>cor_topic_model <span class="ot">&lt;-</span> <span class="fu">stm</span>(myDfm, <span class="at">K =</span> <span class="dv">5</span>,</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">verbose =</span> <span class="cn">FALSE</span>, <span class="at">init.type =</span> <span class="st">"Spectral"</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>cor_topic_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>A topic model with 5 topics, 5000 documents and a 47719 word dictionary.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cor_topic_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>A topic model with 5 topics, 5000 documents and a 47719 word dictionary.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Topic 1 Top Words:
     Highest Prob: film, one, horror, just, even, like, bad 
     FREX: slasher, zombie, zombies, scarecrows, halloween, kornbluth, horror 
     Lift: boogey, btk, cheapo, copycat, dornwinkle, faat, impaled 
     Score: zombie, slasher, horror, scarecrows, kornbluth, zombies, scarecrow 
Topic 2 Top Words:
     Highest Prob: &gt;, &lt;, br, film, one, movie, like 
     FREX: &gt;, &lt;, br, zizek, miya, |, aztec 
     Lift: 102, acre, bolkan, brull, charis, cortes, florinda 
     Score: &gt;, &lt;, br, miya, zizek, slugs, oshii 
Topic 3 Top Words:
     Highest Prob: film, one, story, life, films, also, love 
     FREX: bettie, widmark, sidney, mathieu, vargas, macarthur, chavez 
     Lift: _by, --almost, --five-out-of-ten, --not, --who, --you, -1940s 
     Score: film, bettie, mathieu, widmark, macarthur, chavez, vargas 
Topic 4 Top Words:
     Highest Prob: one, show, good, best, also, film, like 
     FREX: wwe, rochester, triple, blandings, kolchak, spock, taker 
     Lift: _ex_executives, --another, --but, -erica, -filled, -juan, -kelly 
     Score: wwe, taker, bubba, benoit, rochester, booker, kolchak 
Topic 5 Top Words:
     Highest Prob: movie, like, just, one, good, film, really 
     FREX: movie, watched, movies, liked, funny, think, loved 
     Lift: _____, ______, _real_, _tried, --an, --locate, --polarisdib 
     Score: movie, movies, bad, stupid, funny, think, like </code></pre>
</div>
</div>
<p>Once we’ve estimated the model, we’ll want to take a look at the topics. Importantly, we don’t get nice, neat topic names. What we do have are the words that are most frequent, probable, or that otherwise characterize a topic. STM provides handy functionality to extract those words with the <code>labelTopics()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">labelTopics</span>(cor_topic_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Topic 1 Top Words:
     Highest Prob: film, one, horror, just, even, like, bad 
     FREX: slasher, zombie, zombies, scarecrows, halloween, kornbluth, horror 
     Lift: boogey, btk, cheapo, copycat, dornwinkle, faat, impaled 
     Score: zombie, slasher, horror, scarecrows, kornbluth, zombies, scarecrow 
Topic 2 Top Words:
     Highest Prob: &gt;, &lt;, br, film, one, movie, like 
     FREX: &gt;, &lt;, br, zizek, miya, |, aztec 
     Lift: 102, acre, bolkan, brull, charis, cortes, florinda 
     Score: &gt;, &lt;, br, miya, zizek, slugs, oshii 
Topic 3 Top Words:
     Highest Prob: film, one, story, life, films, also, love 
     FREX: bettie, widmark, sidney, mathieu, vargas, macarthur, chavez 
     Lift: _by, --almost, --five-out-of-ten, --not, --who, --you, -1940s 
     Score: film, bettie, mathieu, widmark, macarthur, chavez, vargas 
Topic 4 Top Words:
     Highest Prob: one, show, good, best, also, film, like 
     FREX: wwe, rochester, triple, blandings, kolchak, spock, taker 
     Lift: _ex_executives, --another, --but, -erica, -filled, -juan, -kelly 
     Score: wwe, taker, bubba, benoit, rochester, booker, kolchak 
Topic 5 Top Words:
     Highest Prob: movie, like, just, one, good, film, really 
     FREX: movie, watched, movies, liked, funny, think, loved 
     Lift: _____, ______, _real_, _tried, --an, --locate, --polarisdib 
     Score: movie, movies, bad, stupid, funny, think, like </code></pre>
</div>
</div>
<p>We can also look at the top documents associated with each topic using <code>findThoughts()</code>. Here, we’ll look at the top document (<code>n=1</code>) for each of the 5 topics (<code>topics = c(1:5)</code>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="fu">findThoughts</span>(cor_topic_model,</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">texts =</span> movie_review<span class="sc">$</span>review,</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">topics =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>),</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">n =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 Topic 1: 
     Need a lesson in pure, abject failure?? Look no further than \"Wizards of the Lost Kingdom\", an abysmal, dirt-poor, disgrace of a flick. As we all know, decent moovies tend to sprout horrible, horrible offspring: \"Halloween\" begat many, many bad 80's slasher flicks; \"Mad Max\" begat many, many bad 80's \"futuristic wasteland fantasy\" flicks; and \"Conan the Barbarian\" begat a whole slew of terrible, horrible, incredibly bad 80's sword-and-sorcery flicks. \"Wizards of the Lost Kingdom\" scrapes the bottom of that 80's barrel, in a way that's truly insulting to barrels. A young runt named Simon recaptured his \"good kingdom\" from an evil sorcerer with the help of a mangy rug, a garden gnome, a topless bimbo mermaid, and a tired-looking, pudgy Bo Svenson. Svenson(\"North Dallas Forty\", \"Inglorious Bastards\", \"Delta Force\"), a long-time b-moovie muscleman, looks barely able to swing his aluminum foil sword. However, he manages to defeat the forces of evil, which consist of the evil sorcerer, \"Shurka\", and his army of badly costumed monsters, giants, and midgets. At one point, a paper mache bat on a string attacks, but is eaten by a 1/2 hidden sock puppet, pitifully presented as some sort of dragon. The beginning of the film consists of what can only politely be described as bits of scenes scooped up from the cutting-room floor of udder bad moovies, stitched together in the vain hope of setting the scene for the film, and over-earnestly narrated by some guy who never appears again. Words cannot properly convey the jaw-dropping cheapness of this film; the producers probably spent moore moolah feeding Svenson's ever expanding gullet than on the cheesy fx of this flick. And we're talkin' Brie here, folks... :=8P Director Hector Olivera(\"Barbarian Queen\") presents this mish-mash in a hopelessly confused, confuddled, and cliched manner, destroying any possible hint of clear, linear storytelling. The acting is dreadful, the production levels below shoe-string, and the plot is one tired cliche after another paraded before our weary eyes. That they actually made a sequel(!!!) makes the MooCow's brain whirl. James Horner's(\"Braveheart\", \"Titanic\",\"The Rock\") cheesy moosic from \"Battle Beyond the Stars\" was lifted, screaming and kicking, and mercilessly grafted onto this turkey - bet this one doesn't pop up on his resume. Folks, you gotta see this to believe it. The MooCow says as a cheapo rent when there is NOTHING else to watch, well, it's moore fun than watching dust bunnies mate. Barely. :=8P 
 Topic 2: 
     David Arquette is a young and naive home security alarm&lt;br /&gt;&lt;br /&gt;salesman taken under the wing of Stanley Tucci. Arquette is a&lt;br /&gt;&lt;br /&gt;golden boy, scoring a big sale on his first call- to widow Kate&lt;br /&gt;&lt;br /&gt;Capshaw and her dopey son Ryan Reynolds. Things are going&lt;br /&gt;&lt;br /&gt;well for Arquette, he is appearing in commercials for the security&lt;br /&gt;&lt;br /&gt;firm and he is falling in love with Capshaw.&lt;br /&gt;&lt;br /&gt;Then Tucci and his right hand woman Mary McCormack let him in&lt;br /&gt;&lt;br /&gt;on a little secret- they sometimes break into the houses of their&lt;br /&gt;&lt;br /&gt;clients in order to scare them and to get their neighbors to buy&lt;br /&gt;&lt;br /&gt;security systems from the firm. Arquette decides not to get&lt;br /&gt;&lt;br /&gt;involved, taking Capshaw to meet his family, and going through life&lt;br /&gt;&lt;br /&gt;with a goofy smile on his face. Then, someone breaks into&lt;br /&gt;&lt;br /&gt;Capshaw's home and murders her and her son. Arquette suspects Tucci, and sets a series of traps, resulting in a gun to his&lt;br /&gt;&lt;br /&gt;boss' head as Tucci pleads his innocence.&lt;br /&gt;&lt;br /&gt;Based on a stage play, \"The Alarmist\" is not opened up well. The&lt;br /&gt;&lt;br /&gt;scenes where Arquette takes the Capshaw to meet his parents&lt;br /&gt;&lt;br /&gt;are badly played and completely unfunny. They are also out of line&lt;br /&gt;&lt;br /&gt;with the character Capshaw is playing, as she gets drunk and tells&lt;br /&gt;&lt;br /&gt;sexually explicit stories to Arquette's mom Michael Learned. Other&lt;br /&gt;&lt;br /&gt;than these scenes, Capshaw is not given much to do, but she&lt;br /&gt;&lt;br /&gt;does a lot with the little she is given.&lt;br /&gt;&lt;br /&gt;Stanley Tucci, looking just like Terry O'Quinn, is a riot as the&lt;br /&gt;&lt;br /&gt;security firm owner. He is a creep who really does not understand&lt;br /&gt;&lt;br /&gt;Arquette's moral revulsion. However, when he turns into a&lt;br /&gt;&lt;br /&gt;sniveling whiner after Arquette kidnaps him, he is hilarious. Mary&lt;br /&gt;&lt;br /&gt;McCormack seems to have been groomed for a bigger role, but&lt;br /&gt;&lt;br /&gt;she mostly stands around and agrees with Tucci. Ryan Reynolds&lt;br /&gt;&lt;br /&gt;is too old to play a dumb teenager, but he is funny, especially&lt;br /&gt;&lt;br /&gt;telling his own explicit sexual story to Arquette.&lt;br /&gt;&lt;br /&gt;The screenplay lurches from romantic comedy to dark comedy too&lt;br /&gt;&lt;br /&gt;soon. Capshaw meeting the parents is completely unmotivated,&lt;br /&gt;&lt;br /&gt;except to give her a reason to get out of town so someone can&lt;br /&gt;&lt;br /&gt;break into her house. Capshaw and Reynolds are in the film just&lt;br /&gt;&lt;br /&gt;to give Arquette a reason to take revenge on Tucci.&lt;br /&gt;&lt;br /&gt;Arquette, who has proven he is a good actor, is awful here. He&lt;br /&gt;&lt;br /&gt;relies on the constipated mugging that got him through those&lt;br /&gt;&lt;br /&gt;AT&amp;T ads, and he is not a strong enough presence to build this&lt;br /&gt;&lt;br /&gt;weak film around. Actually, Reynolds might have been a better&lt;br /&gt;&lt;br /&gt;choice in the role.&lt;br /&gt;&lt;br /&gt;Dunsky's direction is good, nothing that will win an Oscar soon.&lt;br /&gt;&lt;br /&gt;Christophe Beck's light jazzy score recalls the type of film noir this&lt;br /&gt;&lt;br /&gt;film tries to be, and it is really catchy on top of that.&lt;br /&gt;&lt;br /&gt;Despite the pluses, Arquette's failure as a lead and the script's&lt;br /&gt;&lt;br /&gt;schizophrenic quality sinks the film. I do not recommend it.&lt;br /&gt;&lt;br /&gt;This is rated (R) for physical violence, gun violence, some gore,&lt;br /&gt;&lt;br /&gt;strong profanity, brief female nudity, sexual content, strong sexual&lt;br /&gt;&lt;br /&gt;references, and adult situations. 
 Topic 3: 
     Since most review's of this film are of screening's seen decade's ago I'd like to add a more recent one, the film open's with stock footage of B-17's bombing Germany, the film cut's to Oskar Werner's Hauptmann (captain) Wust character and his aide running for cover while making their way to Hitler's Fuehrer Bunker, once inside, they are debriefed by bunker staff personnel, the film then cut's to one of many conference scene's with Albin Skoda giving a decent impression of Adolf Hitler rallying his officer's to \"Ultimate Victory\" while Werner's character is shown as slowly coming to realize the bunker denizen's are caught up in a fantasy world-some non-bunker event's are depicted, most notable being the flooding of the subway system to prevent a Russian advance through them and a minor subplot involving a young member of the Flak unit's and his family's difficulty in surviving-this film suffer's from a number of detail inaccuracies that a German film made only 10 year's after WW2 should not have included; the actor portraying Goebbels (Willy Krause) wear's the same uniform as Hitler, including arm eagle- Goebbels wore a brown Nazi Party uniform with swastika armband-the \"SS\" soldier's wear German army camouflage, the well documented scene of Hitler awarding the iron cross to boy's of the Hitler Youth is shown as having taken place INSIDE the bunker (it was done outside in the courtyard) and lastly, Hitler's suicide weapon is clearly shown as a Belgian browning model 1922-most account's agree it was a Walther PPK-some bit's of acting also seem wholly inaccurate with the drunken dance scene near the end of the film being notable, this bit is shown as a cabaret skit, with a intoxicated wounded soldier (his arm in a splint) maniacally goose-stepping to music while a nurse does a combination striptease/belly dance, all by candlelight... this is actually embarrassing to watch-the most incredible bit is when Werner's Captain Wust gain's an audience alone with Skoda's Hitler, Hitler is shown as slumped on a wall bench, drugged and delirious, when Werner's character begin's to question him, Hitler start's screaming which bring's in a SS guard who mortally wound's Werner's character in the back with a gunshot-this fabricated scene is not based on any true historic account-Werner's character is then hauled off to die in a anteroom while Hitler prepare's his own ending, Hitler's farewell to his staff is shown but the suicide is off-screen, the final second's of the movie show Hitler's funeral pyre smoke slowly forming into a ghostly image of the face of the dead Oskar Werner/Hauptmann Wust-this film is more allegorical than historical and anyone interested in this period would do better to check out more recent film's such as the 1973 remake \"Hitler: the last 10 day's\" or the German film \"Downfall\" (Der Untergang) if they wish a more true accounting of this dramatic story, these last two film's are based on first person eyewitness account's, with \"Hitler: the last 10 day's\" being compiled from Gerhard Boldt's autobiography as a staff officer in the Fuehrer Bunker and \"Downfall\" being done from Hitler's secretary's recollection's, the screen play for \"Der Letzte Akte\" is taken from American Nuremberg war crime's trial judge Michael Musmanno's book \"Ten day's to die\", which is more a compilation of event's (many obviously fanciful) than eyewitness history-it is surprising that Hugh Trevor Roper's account,\"The last day's of Hitler\" was never made into a film. 
 Topic 4: 
     This happy-go-luck 1939 military swashbuckler, based rather loosely on Rudyard Kipling's memorable poem as well as his novel \"Soldiers Three,\" qualifies as first-rate entertainment about the British Imperial Army in India in the 1880s. Cary Grant delivers more knock-about blows with his knuckled-up fists than he did in all of his movies put together. Set in faraway India, this six-fisted yarn dwells on the exploits of three rugged British sergeants and their native water bearer Gunga Din (Sam Jaffe) who contend with a bloodthirsty cult of murderous Indians called the Thuggee. Sergeant Archibald Cutter (Cary Grant of \"The Last Outpost\"), Sergeant MacChesney (Oscar-winner Victor McLaglen of \"The Informer\"), and Sergeant Ballantine (Douglas Fairbanks, Jr. of \"The Dawn Patrol\"), are a competitive trio of hard-drinking, hard-brawling, and fun-loving Alpha males whose years of frolic are about to become history because Ballantine plans to marry Emmy Stebbins (Joan Fontaine) and enter the tea business. Naturally, Cutter and MacChesney drum up assorted schemes to derail Ballentine's plans. When their superiors order them back into action with Sgt. Bertie Higginbotham (Robert Coote of \"The Sheik Steps Out\"), Cutter and MacChesney drug Higginbotham so that he cannot accompany them and Ballantine has to replace him. Half of the fun here is watching the principals trying to outwit each other without hating themselves. Director George Stevens celebrates the spirit of adventure in grand style and scope as our heroes tangle with an army of Thuggees. Lenser Joseph H. August received an Oscar nomination for his outstanding black &amp; white cinematography. 
 Topic 5: 
     Full House is a great show. I am still today growing up on it. I started watching it when i was 8 and now i am 12 and still watching it. i fell in love with all of the characters, especially Stephanie. she is my favorite. she had such a sense of humor. in case there are people on this sight that hardly watch the show, you should because you will get hooked on it. i became hooked on it after the first show i saw, which just happened to be the first episode, in 2002. it really is a good show. i really think that this show should go down to many generations in families. and it's great too because it is an appropriate show for all ages. and for all parents, it teaches kids lessons on how to go on with their life. nothing terrible happens, like violence or swearing. it is just a really great sit-com. i give it 5 out of 5 stars. what do you think? OH and the best time to watch it is when you are home sick from school or even the old office. It will make you feel a lot better. Trust me i am hardly home sick but i still know that it will make you feel better. and to everybody that thinks the show is stupid, well that's too bad for you because you won't get as far in life even if you are happy with your life. you really should watch it and you will get hooked on it. i am just telling you what happened to me and everybody else that started watching this awesome show. well i need must go to have some lunch. remember you must start watching full house and soon!</code></pre>
</div>
</div>
</section>
<section id="structural-topic-model-1" class="level3">
<h3 class="anchored" data-anchor-id="structural-topic-model-1">Structural Topic model</h3>
<p>Let’s go ahead and estimate our structural topic model now. We’ll incorporate the <code>sentiment</code> variable as a predictor on prevalence.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># choose our number of topics</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="co"># specify model</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>myModel <span class="ot">&lt;-</span> <span class="fu">stm</span>(myDfm,</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">K =</span> k,</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">prevalence =</span> <span class="sc">~</span> sentiment,</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> movie_review,</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">max.em.its =</span> <span class="dv">1000</span>,</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">seed =</span> <span class="dv">1234</span>,</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">init.type =</span> <span class="st">"Spectral"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Beginning Spectral Initialization 
     Calculating the gram matrix...
     Using only 10000 most frequent terms during initialization...
     Finding anchor words...
    .....
     Recovering initialization...
    ....................................................................................................
Initialization complete.
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 1 (approx. per word bound = -8.615) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 2 (approx. per word bound = -8.043, relative change = 6.646e-02) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 3 (approx. per word bound = -8.008, relative change = 4.390e-03) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 4 (approx. per word bound = -7.999, relative change = 1.042e-03) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 5 (approx. per word bound = -7.995, relative change = 4.933e-04) 
Topic 1: film, br, &lt;, &gt;, movie 
 Topic 2: &gt;, &lt;, br, film, movie 
 Topic 3: film, br, &lt;, &gt;, one 
 Topic 4: one, br, &lt;, &gt;, show 
 Topic 5: movie, like, one, just, br 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 6 (approx. per word bound = -7.993, relative change = 2.928e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 7 (approx. per word bound = -7.991, relative change = 1.982e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 8 (approx. per word bound = -7.990, relative change = 1.496e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 9 (approx. per word bound = -7.989, relative change = 1.181e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 10 (approx. per word bound = -7.988, relative change = 9.822e-05) 
Topic 1: film, one, just, like, movie 
 Topic 2: &gt;, &lt;, br, film, one 
 Topic 3: film, br, &lt;, &gt;, one 
 Topic 4: one, show, like, good, film 
 Topic 5: movie, like, just, one, good 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 11 (approx. per word bound = -7.988, relative change = 8.465e-05) 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 12 (approx. per word bound = -7.987, relative change = 7.740e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 13 (approx. per word bound = -7.987, relative change = 6.696e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 14 (approx. per word bound = -7.986, relative change = 5.219e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 15 (approx. per word bound = -7.986, relative change = 4.341e-05) 
Topic 1: film, one, just, like, even 
 Topic 2: &gt;, &lt;, br, film, one 
 Topic 3: film, one, story, br, &lt; 
 Topic 4: one, show, good, like, film 
 Topic 5: movie, like, just, one, good 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 16 (approx. per word bound = -7.986, relative change = 3.805e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 17 (approx. per word bound = -7.985, relative change = 3.305e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 18 (approx. per word bound = -7.985, relative change = 2.854e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 19 (approx. per word bound = -7.985, relative change = 2.753e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 20 (approx. per word bound = -7.985, relative change = 2.927e-05) 
Topic 1: film, one, just, even, like 
 Topic 2: &gt;, &lt;, br, film, one 
 Topic 3: film, one, story, life, films 
 Topic 4: one, show, good, film, best 
 Topic 5: movie, like, just, one, good 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 21 (approx. per word bound = -7.984, relative change = 3.115e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 22 (approx. per word bound = -7.984, relative change = 2.915e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 23 (approx. per word bound = -7.984, relative change = 2.367e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 24 (approx. per word bound = -7.984, relative change = 2.057e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 25 (approx. per word bound = -7.984, relative change = 2.056e-05) 
Topic 1: film, one, just, even, like 
 Topic 2: &gt;, &lt;, br, film, one 
 Topic 3: film, one, story, life, films 
 Topic 4: one, show, good, best, film 
 Topic 5: movie, like, just, one, good 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 26 (approx. per word bound = -7.983, relative change = 2.021e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 27 (approx. per word bound = -7.983, relative change = 1.789e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 28 (approx. per word bound = -7.983, relative change = 1.564e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 29 (approx. per word bound = -7.983, relative change = 1.315e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 30 (approx. per word bound = -7.983, relative change = 1.195e-05) 
Topic 1: film, one, just, even, like 
 Topic 2: &gt;, &lt;, br, film, one 
 Topic 3: film, one, story, life, films 
 Topic 4: one, show, good, best, film 
 Topic 5: movie, like, just, one, good 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 31 (approx. per word bound = -7.983, relative change = 1.065e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 32 (approx. per word bound = -7.983, relative change = 1.092e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 33 (approx. per word bound = -7.983, relative change = 1.172e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 34 (approx. per word bound = -7.983, relative change = 1.203e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 35 (approx. per word bound = -7.983, relative change = 1.252e-05) 
Topic 1: film, one, just, even, like 
 Topic 2: &gt;, &lt;, br, film, one 
 Topic 3: film, one, story, life, films 
 Topic 4: one, show, good, film, best 
 Topic 5: movie, like, just, one, good 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 36 (approx. per word bound = -7.982, relative change = 1.298e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 37 (approx. per word bound = -7.982, relative change = 1.139e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Model Converged </code></pre>
</div>
</div>
<p>Note what’s significantly different from before is added the <code>prevalence</code> formula. As we discuss in lecture, you can also include variables as <code>content</code> predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">labelTopics</span>(myModel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Topic 1 Top Words:
     Highest Prob: film, one, just, even, like, bad, horror 
     FREX: slasher, scarecrows, zombie, zombies, kornbluth, scarecrow, seagal 
     Lift: -overall, 2600, addy, antichrist, aranoa, ba, bale 
     Score: slasher, zombie, scarecrows, kornbluth, zombies, horror, bad 
Topic 2 Top Words:
     Highest Prob: &gt;, &lt;, br, film, one, movie, like 
     FREX: &gt;, &lt;, br, zizek, miya, |, aztec 
     Lift: ------------, --------------, ----------------, -------------------, ----------------------------------------, -buster, -contest 
     Score: &gt;, &lt;, br, zizek, miya, slugs, oshii 
Topic 3 Top Words:
     Highest Prob: film, one, story, life, films, also, love 
     FREX: bettie, widmark, sidney, mathieu, macarthur, chavez, israel 
     Lift: lumet, _by, --five-out-of-ten, --not, --who, --you, -1940s 
     Score: film, bettie, mathieu, widmark, macarthur, aids, antwone 
Topic 4 Top Words:
     Highest Prob: one, show, film, good, best, also, like 
     FREX: wwe, rochester, blandings, kolchak, spock, taker, dominick 
     Lift: 1692, 1931, absurdist, adrien, adversaries, affirmative, alaric 
     Score: wwe, taker, bubba, benoit, booker, rochester, kolchak 
Topic 5 Top Words:
     Highest Prob: movie, like, just, one, good, film, really 
     FREX: movie, watched, movies, liked, funny, loved, kids 
     Lift: affiliated, ai, ammo, anka, ardala, b5, bauraki 
     Score: movie, movies, bad, stupid, funny, think, like </code></pre>
</div>
</div>
<p>The topics again look reasonable, and are generally similar to the topics we estimated earlier. We can go a step further by plotting out the top topics (as groups of words associated with that topic) and their estimated frequency across the corpus.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myModel, <span class="at">type =</span> <span class="st">"summary"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Tutorial10_files/figure-html/unnamed-chunk-39-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>One thing we might want to do is to extract the topics and to assign them to the vector of document proportions; this is often useful if we’re using those topic proportions in any sort of downstream analysis, including just a visualization. The following extracts the top words (here, by <code>frex</code>, though you can update that to any of the other three top word sets). Then it iterates through the extracted sets and collapses the strings so the tokens are separated by an underscore; this is useful as a variable name for those downstream analyses.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the words</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>myTopicNames <span class="ot">&lt;-</span> <span class="fu">labelTopics</span>(myModel, <span class="at">n=</span><span class="dv">4</span>)<span class="sc">$</span>frex</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set up an empty vector</span></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>myTopicLabels <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, k)</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="co"># set up a loop to go through the topics and collapse the words to a single name</span></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>  myTopicLabels[i] <span class="ot">&lt;-</span> <span class="fu">paste</span>(myTopicNames[i,], <span class="at">collapse =</span> <span class="st">"_"</span>)</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a><span class="co"># print the names</span></span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>myTopicLabels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "slasher_scarecrows_zombie_zombies" "&gt;_&lt;_br_zizek"                     
[3] "bettie_widmark_sidney_mathieu"     "wwe_rochester_blandings_kolchak"  
[5] "movie_watched_movies_liked"       </code></pre>
</div>
</div>
</section>
<section id="estimate-effect" class="level3">
<h3 class="anchored" data-anchor-id="estimate-effect">Estimate Effect</h3>
<p>Recall that we included <code>sentiment</code> as a predictor variable on topical prevalence. We can extract the effect of the predictor here using the <code>estimateEffect()</code> function, which takes as arguments a formula, the stm model object, and the metadata containing the predictor variable.</p>
<p>Once we’ve run the function, we can plot the estimated effects of <code>sentiment</code> on topic prevalence for each of the estimated topics. With a dichotomous predictor variable, we’ll plot these out solely as the difference (<code>method = "difference"</code>) in topic prevalence across the values of the predictor. Here, our estimate indicates how much more (or less) the topic is discussed when the sentiment of the post is positive.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate effects</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>modelEffects <span class="ot">&lt;-</span> <span class="fu">estimateEffect</span>(<span class="at">formula =</span> <span class="dv">1</span><span class="sc">:</span>k <span class="sc">~</span> sentiment,</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">stmobj =</span> myModel,</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">metadata =</span> movie_review)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot effects</span></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>myRows <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(myRows, <span class="dv">3</span>), <span class="at">bty =</span> <span class="st">"n"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot.estimateEffect</span>(modelEffects,</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>                      <span class="at">covariate =</span> <span class="st">"sentiment"</span>,</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>                      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">25</span>, .<span class="dv">25</span>),</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>                      <span class="at">model =</span> myModel,</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>                      <span class="at">topics =</span> modelEffects<span class="sc">$</span>topics[i],</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>                      <span class="at">method =</span> <span class="st">"difference"</span>,</span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>                      <span class="at">cov.value1 =</span> <span class="dv">1</span>,</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>                      <span class="at">cov.value2 =</span> <span class="dv">0</span>, </span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>                      <span class="at">main =</span> myTopicLabels[i],</span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>                      <span class="at">printlegend =</span> F,</span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a>                      <span class="at">linecol =</span> <span class="st">"grey26"</span>,</span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labeltype =</span> <span class="st">"custom"</span>,</span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>                      <span class="at">verbose.labels =</span> F,</span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a>                      <span class="at">custom.labels =</span> <span class="fu">c</span>(<span class="st">""</span>))</span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">new =</span> F)</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Tutorial10_files/figure-html/unnamed-chunk-41-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="choosing-k" class="level3">
<h3 class="anchored" data-anchor-id="choosing-k">Choosing K</h3>
<p>I’m sure you were thinking “How did she select 5 topics?” Well, the answer is that it was just a random number that I selected out of thin air. The choice of the number of topics, typically denoted K, is one of the areas where the design of topic models let’s us as researchers down a bit. While some approaches have been proposed, none have really gained traction. STM includes an approach that we won’t explore based on work by David Mimno that automatically identifies a topic; in reality, it normally results in far more topics than a human would be likely to choose.</p>
<p>With all that said, there is some functionality included with STM to explore different specifications and to try to at least get some idea of how different approaches perform. <code>searchK()</code> lets you estimate a series of different models, then you can plot a series of different evaluation metrics across those choices.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>differentKs <span class="ot">&lt;-</span> <span class="fu">searchK</span>(myDfm,</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">K =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">25</span>, <span class="dv">50</span>),</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">prevalence =</span> <span class="sc">~</span> sentiment,</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">N =</span> <span class="dv">250</span>,</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data =</span> movie_review,</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">max.em.its =</span> <span class="dv">1000</span>,</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>                       <span class="at">init.type =</span> <span class="st">"Spectral"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Beginning Spectral Initialization 
     Calculating the gram matrix...
     Using only 10000 most frequent terms during initialization...
     Finding anchor words...
    .....
     Recovering initialization...
    ....................................................................................................
Initialization complete.
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 1 (approx. per word bound = -8.614) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 2 (approx. per word bound = -8.038, relative change = 6.687e-02) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 3 (approx. per word bound = -8.002, relative change = 4.386e-03) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 4 (approx. per word bound = -7.995, relative change = 9.712e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 5 (approx. per word bound = -7.991, relative change = 4.555e-04) 
Topic 1: &gt;, br, &lt;, one, film 
 Topic 2: movie, like, just, one, film 
 Topic 3: &gt;, &lt;, br, film, movie 
 Topic 4: film, &lt;, br, &gt;, one 
 Topic 5: &gt;, br, &lt;, one, show 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 6 (approx. per word bound = -7.989, relative change = 2.730e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 7 (approx. per word bound = -7.987, relative change = 1.882e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 8 (approx. per word bound = -7.986, relative change = 1.442e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 9 (approx. per word bound = -7.985, relative change = 1.026e-04) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 10 (approx. per word bound = -7.985, relative change = 7.521e-05) 
Topic 1: one, film, like, just, characters 
 Topic 2: movie, like, film, just, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, &lt;, br, &gt; 
 Topic 5: &gt;, br, &lt;, one, show 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 11 (approx. per word bound = -7.984, relative change = 5.662e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 12 (approx. per word bound = -7.984, relative change = 4.903e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 13 (approx. per word bound = -7.984, relative change = 4.331e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 14 (approx. per word bound = -7.983, relative change = 3.716e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 15 (approx. per word bound = -7.983, relative change = 3.185e-05) 
Topic 1: film, one, like, just, characters 
 Topic 2: movie, film, like, just, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, &lt;, br 
 Topic 5: &gt;, br, &lt;, one, show 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 16 (approx. per word bound = -7.983, relative change = 2.919e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 17 (approx. per word bound = -7.983, relative change = 2.689e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 18 (approx. per word bound = -7.982, relative change = 2.472e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 19 (approx. per word bound = -7.982, relative change = 2.209e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 20 (approx. per word bound = -7.982, relative change = 2.178e-05) 
Topic 1: film, one, like, just, characters 
 Topic 2: movie, film, like, just, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, life, also 
 Topic 5: &gt;, br, &lt;, one, show 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 21 (approx. per word bound = -7.982, relative change = 2.088e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 22 (approx. per word bound = -7.982, relative change = 2.045e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 23 (approx. per word bound = -7.982, relative change = 2.134e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 24 (approx. per word bound = -7.981, relative change = 1.825e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 25 (approx. per word bound = -7.981, relative change = 1.355e-05) 
Topic 1: film, one, like, just, characters 
 Topic 2: movie, film, like, just, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, life, also 
 Topic 5: one, &gt;, br, &lt;, show 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 26 (approx. per word bound = -7.981, relative change = 1.156e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Completing Iteration 27 (approx. per word bound = -7.981, relative change = 1.014e-05) 
....................................................................................................
Completed E-Step (1 seconds). 
Completed M-Step. 
Model Converged 
Beginning Spectral Initialization 
     Calculating the gram matrix...
     Using only 10000 most frequent terms during initialization...
     Finding anchor words...
    .........................
     Recovering initialization...
    ....................................................................................................
Initialization complete.
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 1 (approx. per word bound = -8.589) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 2 (approx. per word bound = -7.792, relative change = 9.278e-02) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 3 (approx. per word bound = -7.708, relative change = 1.084e-02) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 4 (approx. per word bound = -7.691, relative change = 2.256e-03) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 5 (approx. per word bound = -7.683, relative change = 9.445e-04) 
Topic 1: film, &lt;, br, &gt;, characters 
 Topic 2: movie, film, good, actors, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, br, &lt; 
 Topic 5: show, one, episode, best, series 
 Topic 6: movie, great, one, film, made 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, &lt;, &gt;, br, one 
 Topic 9: br, &lt;, &gt;, film, one 
 Topic 10: movie, film, got, like, one 
 Topic 11: br, &lt;, &gt;, good, time 
 Topic 12: br, &gt;, &lt;, film, one 
 Topic 13: br, &gt;, &lt;, film, one 
 Topic 14: movie, &gt;, &lt;, br, just 
 Topic 15: br, &gt;, &lt;, film, one 
 Topic 16: br, &lt;, &gt;, movie, film 
 Topic 17: movie, bad, film, good, just 
 Topic 18: movie, like, show, br, &lt; 
 Topic 19: &gt;, br, &lt;, movie, film 
 Topic 20: &gt;, &lt;, br, one, film 
 Topic 21: &lt;, br, one, &gt;, movie 
 Topic 22: film, &gt;, br, &lt;, just 
 Topic 23: &lt;, br, &gt;, new, one 
 Topic 24: br, &gt;, &lt;, movie, film 
 Topic 25: &gt;, br, &lt;, documentary, film 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 6 (approx. per word bound = -7.679, relative change = 5.183e-04) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 7 (approx. per word bound = -7.677, relative change = 3.323e-04) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 8 (approx. per word bound = -7.675, relative change = 2.341e-04) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 9 (approx. per word bound = -7.674, relative change = 1.762e-04) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 10 (approx. per word bound = -7.673, relative change = 1.404e-04) 
Topic 1: film, characters, one, &lt;, br 
 Topic 2: movie, film, good, actors, story 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, films, excellent 
 Topic 5: show, one, episode, best, series 
 Topic 6: movie, great, one, film, made 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, &lt;, &gt;, br, one 
 Topic 9: film, br, &lt;, &gt;, one 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, time, film, one, br 
 Topic 12: br, &gt;, &lt;, film, one 
 Topic 13: br, &lt;, &gt;, film, one 
 Topic 14: movie, just, bad, like, see 
 Topic 15: br, &gt;, &lt;, film, one 
 Topic 16: movie, film, br, &lt;, &gt; 
 Topic 17: film, movie, bad, good, just 
 Topic 18: movie, like, show, just, one 
 Topic 19: &gt;, br, &lt;, movie, film 
 Topic 20: &gt;, &lt;, br, one, film 
 Topic 21: one, movie, &lt;, br, &gt; 
 Topic 22: film, just, one, &gt;, br 
 Topic 23: &lt;, br, &gt;, new, film 
 Topic 24: movie, br, &gt;, &lt;, film 
 Topic 25: documentary, war, &gt;, film, br 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 11 (approx. per word bound = -7.672, relative change = 1.166e-04) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 12 (approx. per word bound = -7.671, relative change = 1.016e-04) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 13 (approx. per word bound = -7.670, relative change = 8.924e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 14 (approx. per word bound = -7.670, relative change = 8.016e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 15 (approx. per word bound = -7.669, relative change = 7.067e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, actors, story 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, films, excellent 
 Topic 5: show, one, episode, best, series 
 Topic 6: movie, great, one, film, good 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, &lt;, &gt;, br 
 Topic 9: film, br, &lt;, &gt;, one 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, time, film, one, story 
 Topic 12: br, &gt;, &lt;, film, one 
 Topic 13: br, &lt;, &gt;, film, one 
 Topic 14: movie, just, bad, like, see 
 Topic 15: film, one, much, like, br 
 Topic 16: movie, film, one, just, like 
 Topic 17: film, bad, movie, just, good 
 Topic 18: movie, like, show, just, tv 
 Topic 19: &gt;, br, &lt;, film, movie 
 Topic 20: one, film, &gt;, br, &lt; 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, like, films 
 Topic 23: &lt;, br, &gt;, new, film 
 Topic 24: movie, film, love, br, &gt; 
 Topic 25: war, documentary, film, people, &gt; 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 16 (approx. per word bound = -7.669, relative change = 6.158e-05) 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 17 (approx. per word bound = -7.668, relative change = 5.699e-05) 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 18 (approx. per word bound = -7.668, relative change = 5.321e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 19 (approx. per word bound = -7.667, relative change = 5.145e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 20 (approx. per word bound = -7.667, relative change = 4.434e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, actors 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, films, woman 
 Topic 5: show, one, best, episode, series 
 Topic 6: movie, great, one, film, good 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, also 
 Topic 9: film, br, &lt;, &gt;, one 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, time, film, one, story 
 Topic 12: film, br, &gt;, &lt;, one 
 Topic 13: film, &lt;, br, &gt;, one 
 Topic 14: movie, just, bad, like, see 
 Topic 15: film, one, much, like, even 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: movie, like, show, just, tv 
 Topic 19: &gt;, br, &lt;, film, movie 
 Topic 20: one, film, match, &gt;, br 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, &lt;, br, &gt;, one 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, documentary, film, people, world 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 21 (approx. per word bound = -7.667, relative change = 4.375e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 22 (approx. per word bound = -7.666, relative change = 4.277e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 23 (approx. per word bound = -7.666, relative change = 4.140e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 24 (approx. per word bound = -7.666, relative change = 3.821e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 25 (approx. per word bound = -7.665, relative change = 3.679e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, actors 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, films, woman 
 Topic 5: show, one, best, episode, series 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, also 
 Topic 9: film, br, &lt;, &gt;, one 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, story 
 Topic 12: film, one, life, br, &gt; 
 Topic 13: film, one, films, &lt;, br 
 Topic 14: movie, just, bad, like, see 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: movie, show, like, just, tv 
 Topic 19: film, movie, one, &gt;, much 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, &lt; 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, documentary, people, film, world 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 26 (approx. per word bound = -7.665, relative change = 3.407e-05) 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 27 (approx. per word bound = -7.665, relative change = 3.341e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 28 (approx. per word bound = -7.665, relative change = 3.305e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 29 (approx. per word bound = -7.664, relative change = 3.277e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 30 (approx. per word bound = -7.664, relative change = 3.064e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, actors 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, films, woman 
 Topic 5: one, show, best, episode, series 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, also 
 Topic 9: film, one, br, &lt;, family 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, show 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, movie, like, just, tv 
 Topic 19: film, movie, one, much, &gt; 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, documentary, people, world, film 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 31 (approx. per word bound = -7.664, relative change = 2.779e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 32 (approx. per word bound = -7.664, relative change = 2.152e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 33 (approx. per word bound = -7.664, relative change = 1.999e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 34 (approx. per word bound = -7.663, relative change = 1.927e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 35 (approx. per word bound = -7.663, relative change = 2.126e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, films, woman 
 Topic 5: one, show, best, episode, character 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, also 
 Topic 9: film, one, family, br, &lt; 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, show 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, movie, like, just, tv 
 Topic 19: film, movie, one, much, game 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, documentary, people, us, world 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 36 (approx. per word bound = -7.663, relative change = 2.180e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 37 (approx. per word bound = -7.663, relative change = 2.064e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 38 (approx. per word bound = -7.663, relative change = 1.567e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 39 (approx. per word bound = -7.663, relative change = 1.952e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 40 (approx. per word bound = -7.663, relative change = 1.663e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, woman, films 
 Topic 5: one, show, best, character, episode 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, just 
 Topic 9: film, one, family, br, &lt; 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, young 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, movie, like, just, tv 
 Topic 19: film, movie, one, much, game 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, people, documentary, us, film 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 41 (approx. per word bound = -7.662, relative change = 1.739e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 42 (approx. per word bound = -7.662, relative change = 1.755e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 43 (approx. per word bound = -7.662, relative change = 2.082e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 44 (approx. per word bound = -7.662, relative change = 2.056e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 45 (approx. per word bound = -7.662, relative change = 1.820e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, woman, films 
 Topic 5: one, best, show, character, episode 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, just 
 Topic 9: film, one, family, story, love 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, young 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, movie, like, just, tv 
 Topic 19: film, movie, one, much, game 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, people, us, documentary, film 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 46 (approx. per word bound = -7.662, relative change = 1.720e-05) 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 47 (approx. per word bound = -7.662, relative change = 1.593e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 48 (approx. per word bound = -7.661, relative change = 1.451e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 49 (approx. per word bound = -7.661, relative change = 1.298e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 50 (approx. per word bound = -7.661, relative change = 1.353e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, woman, films 
 Topic 5: one, best, show, character, series 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, just 
 Topic 9: film, one, family, story, love 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, young 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, like, movie, just, tv 
 Topic 19: film, movie, one, much, game 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, people, us, documentary, film 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 51 (approx. per word bound = -7.661, relative change = 1.297e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 52 (approx. per word bound = -7.661, relative change = 1.293e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 53 (approx. per word bound = -7.661, relative change = 1.279e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 54 (approx. per word bound = -7.661, relative change = 1.184e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 55 (approx. per word bound = -7.661, relative change = 1.212e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, great 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, woman, films 
 Topic 5: one, best, show, character, series 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, just 
 Topic 9: film, one, family, love, story 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, young 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, like, movie, just, tv 
 Topic 19: film, movie, one, much, game 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, people, us, documentary, film 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 56 (approx. per word bound = -7.661, relative change = 1.230e-05) 
....................................................................................................
Completed E-Step (2 seconds). 
Completed M-Step. 
Completing Iteration 57 (approx. per word bound = -7.661, relative change = 1.332e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 58 (approx. per word bound = -7.660, relative change = 1.211e-05) 
....................................................................................................
Completed E-Step (4 seconds). 
Completed M-Step. 
Completing Iteration 59 (approx. per word bound = -7.660, relative change = 1.078e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 60 (approx. per word bound = -7.660, relative change = 1.069e-05) 
Topic 1: film, characters, one, like, just 
 Topic 2: movie, film, good, story, great 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, one, story, woman, films 
 Topic 5: one, best, character, show, series 
 Topic 6: movie, great, one, film, also 
 Topic 7: movie, film, good, one, interesting 
 Topic 8: film, one, like, story, just 
 Topic 9: film, one, family, love, story 
 Topic 10: movie, film, got, like, one 
 Topic 11: good, film, time, one, even 
 Topic 12: film, one, life, like, young 
 Topic 13: film, one, films, like, movie 
 Topic 14: movie, bad, just, like, even 
 Topic 15: film, one, much, like, zombie 
 Topic 16: film, movie, one, just, like 
 Topic 17: film, bad, movie, just, one 
 Topic 18: show, like, movie, just, tv 
 Topic 19: film, movie, one, much, game 
 Topic 20: one, film, match, man, also 
 Topic 21: one, movie, film, movies, seen 
 Topic 22: film, just, one, films, like 
 Topic 23: new, one, film, movie, like 
 Topic 24: movie, film, love, one, s 
 Topic 25: war, people, us, film, documentary 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 61 (approx. per word bound = -7.660, relative change = 1.086e-05) 
....................................................................................................
Completed E-Step (3 seconds). 
Completed M-Step. 
Completing Iteration 62 (approx. per word bound = -7.660, relative change = 1.030e-05) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Model Converged 
Beginning Spectral Initialization 
     Calculating the gram matrix...
     Using only 10000 most frequent terms during initialization...
     Finding anchor words...
    ..................................................
     Recovering initialization...
    ....................................................................................................
Initialization complete.
....................................................................................................
Completed E-Step (11 seconds). 
Completed M-Step. 
Completing Iteration 1 (approx. per word bound = -8.567) 
....................................................................................................
Completed E-Step (9 seconds). 
Completed M-Step. 
Completing Iteration 2 (approx. per word bound = -7.656, relative change = 1.064e-01) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 3 (approx. per word bound = -7.526, relative change = 1.699e-02) 
....................................................................................................
Completed E-Step (7 seconds). 
Completed M-Step. 
Completing Iteration 4 (approx. per word bound = -7.491, relative change = 4.614e-03) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 5 (approx. per word bound = -7.475, relative change = 2.157e-03) 
Topic 1: film, characters, br, &lt;, &gt; 
 Topic 2: movie, actors, director, disappointed, good 
 Topic 3: &lt;, &gt;, br, film, one 
 Topic 4: film, excellent, woman, makes, one 
 Topic 5: best, character, episode, one, show 
 Topic 6: movie, great, one, made, story 
 Topic 7: movie, film, interesting, good, really 
 Topic 8: film, think, br, one, &lt; 
 Topic 9: film, &lt;, br, &gt;, family 
 Topic 10: movie, film, never, story, got 
 Topic 11: time, good, one, film, actually 
 Topic 12: br, &gt;, &lt;, film, one 
 Topic 13: br, &gt;, &lt;, film, lost 
 Topic 14: movie, &gt;, br, &lt;, just 
 Topic 15: &gt;, br, &lt;, film, one 
 Topic 16: movie, just, one, like, film 
 Topic 17: movie, bad, get, one, just 
 Topic 18: movie, like, just, tv, good 
 Topic 19: &gt;, br, &lt;, movie, much 
 Topic 20: br, &gt;, &lt;, match, one 
 Topic 21: movie, one, movies, &gt;, br 
 Topic 22: film, just, &gt;, &lt;, br 
 Topic 23: &lt;, br, &gt;, new, one 
 Topic 24: movie, &gt;, br, &lt;, love 
 Topic 25: &gt;, br, &lt;, movie, like 
 Topic 26: film, can, good, like, see 
 Topic 27: br, &gt;, &lt;, series, original 
 Topic 28: &gt;, &lt;, br, one, movie 
 Topic 29: film, br, &gt;, &lt;, films 
 Topic 30: one, like, movie, &lt;, br 
 Topic 31: good, movie, bad, film, &lt; 
 Topic 32: movie, just, like, really, one 
 Topic 33: &gt;, br, &lt;, movie, see 
 Topic 34: film, horror, br, &gt;, &lt; 
 Topic 35: &lt;, &gt;, br, people, like 
 Topic 36: &gt;, br, &lt;, like, one 
 Topic 37: movie, director, actors, &gt;, br 
 Topic 38: show, &gt;, br, &lt;, one 
 Topic 39: book, movie, &lt;, br, &gt; 
 Topic 40: film, show, br, &gt;, &lt; 
 Topic 41: br, &lt;, &gt;, film, story 
 Topic 42: film, br, &gt;, &lt;, one 
 Topic 43: movie, one, just, br, &gt; 
 Topic 44: br, &gt;, &lt;, show, one 
 Topic 45: film, br, &gt;, &lt;, one 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, &lt;, &gt;, br, one 
 Topic 48: &gt;, br, &lt;, life, movie 
 Topic 49: film, &gt;, br, &lt;, one 
 Topic 50: church, movie, joseph, smith, wife 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 6 (approx. per word bound = -7.466, relative change = 1.232e-03) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 7 (approx. per word bound = -7.460, relative change = 7.705e-04) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 8 (approx. per word bound = -7.456, relative change = 4.905e-04) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 9 (approx. per word bound = -7.454, relative change = 3.398e-04) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 10 (approx. per word bound = -7.452, relative change = 2.431e-04) 
Topic 1: film, characters, script, just, even 
 Topic 2: movie, actors, director, film, disappointed 
 Topic 3: &lt;, &gt;, br, film, one 
 Topic 4: film, excellent, woman, story, one 
 Topic 5: best, character, one, film, episode 
 Topic 6: movie, great, one, made, film 
 Topic 7: movie, film, good, interesting, really 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, &lt;, br, &gt;, family 
 Topic 10: movie, film, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: br, &gt;, &lt;, film, one 
 Topic 13: br, &gt;, &lt;, film, lost 
 Topic 14: movie, &gt;, br, &lt;, see 
 Topic 15: film, one, much, br, &gt; 
 Topic 16: movie, just, one, film, like 
 Topic 17: movie, bad, get, one, like 
 Topic 18: movie, like, just, good, tv 
 Topic 19: &gt;, br, &lt;, movie, film 
 Topic 20: match, br, &gt;, &lt;, one 
 Topic 21: movie, one, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, &lt;, br, &gt; 
 Topic 24: movie, love, film, br, &gt; 
 Topic 25: &gt;, br, &lt;, movie, like 
 Topic 26: film, can, good, see, like 
 Topic 27: series, br, &gt;, &lt;, original 
 Topic 28: one, movie, comedy, film, funny 
 Topic 29: film, films, br, &gt;, &lt; 
 Topic 30: one, like, movie, film, see 
 Topic 31: good, movie, bad, film, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: &gt;, br, &lt;, movie, film 
 Topic 34: film, horror, just, one, films 
 Topic 35: &lt;, &gt;, br, people, documentary 
 Topic 36: &gt;, br, &lt;, like, one 
 Topic 37: movie, director, good, actors, one 
 Topic 38: show, one, episode, like, season 
 Topic 39: book, movie, novel, read, &lt; 
 Topic 40: film, show, like, one, even 
 Topic 41: film, br, &lt;, &gt;, story 
 Topic 42: film, one, br, &gt;, &lt; 
 Topic 43: movie, one, just, film, like 
 Topic 44: br, &gt;, &lt;, show, one 
 Topic 45: film, one, make, heart, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, just, cast 
 Topic 48: &gt;, br, &lt;, life, movie 
 Topic 49: film, one, &gt;, br, &lt; 
 Topic 50: church, smith, movie, joseph, much 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 11 (approx. per word bound = -7.450, relative change = 1.826e-04) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 12 (approx. per word bound = -7.449, relative change = 1.400e-04) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 13 (approx. per word bound = -7.449, relative change = 1.069e-04) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 14 (approx. per word bound = -7.448, relative change = 8.472e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 15 (approx. per word bound = -7.447, relative change = 7.078e-05) 
Topic 1: film, characters, script, just, even 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, excellent, story, one, woman 
 Topic 5: best, character, one, film, great 
 Topic 6: movie, great, one, film, made 
 Topic 7: movie, film, good, interesting, really 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, &lt;, br, &gt;, family 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, show, may 
 Topic 13: film, lost, br, one, &gt; 
 Topic 14: movie, see, just, bad, like 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, just, film, one, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, good, tv 
 Topic 19: &gt;, br, &lt;, movie, film 
 Topic 20: match, one, br, &lt;, &gt; 
 Topic 21: movie, one, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, movie 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, see, like 
 Topic 27: series, original, like, one, film 
 Topic 28: one, movie, comedy, film, funny 
 Topic 29: film, films, one, best, br 
 Topic 30: one, like, movie, film, see 
 Topic 31: good, movie, film, bad, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: movie, &gt;, br, &lt;, film 
 Topic 34: film, horror, one, just, films 
 Topic 35: &lt;, &gt;, br, people, documentary 
 Topic 36: &gt;, br, &lt;, like, one 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, season 
 Topic 39: book, movie, novel, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, br, &lt;, &gt; 
 Topic 42: film, one, get, just, real 
 Topic 43: movie, one, just, film, like 
 Topic 44: show, one, film, br, &gt; 
 Topic 45: film, one, heart, make, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, just, cast 
 Topic 48: &gt;, br, &lt;, life, movie 
 Topic 49: film, one, like, hero, films 
 Topic 50: church, movie, smith, joseph, life 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 16 (approx. per word bound = -7.447, relative change = 5.617e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 17 (approx. per word bound = -7.447, relative change = 4.848e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 18 (approx. per word bound = -7.446, relative change = 5.124e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 19 (approx. per word bound = -7.446, relative change = 5.808e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 20 (approx. per word bound = -7.445, relative change = 5.292e-05) 
Topic 1: film, characters, script, even, just 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, excellent, story, one, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: movie, great, one, film, story 
 Topic 7: movie, film, good, interesting, really 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, &lt;, br, &gt;, family 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, show 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, just, people 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, just, one, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, good, tv 
 Topic 19: &gt;, br, &lt;, film, movie 
 Topic 20: match, one, rock, wwe, ring 
 Topic 21: one, movie, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, movie 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, like, see 
 Topic 27: series, original, like, film, one 
 Topic 28: one, comedy, film, movie, funny 
 Topic 29: film, films, one, best, much 
 Topic 30: one, like, movie, film, see 
 Topic 31: good, movie, film, bad, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: movie, film, see, also, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: &lt;, &gt;, br, people, documentary 
 Topic 36: &gt;, br, &lt;, like, one 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, movie, novel, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, one, role, br 
 Topic 42: film, one, get, real, just 
 Topic 43: movie, one, just, film, like 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, cast, just 
 Topic 48: &gt;, br, &lt;, life, movie 
 Topic 49: film, one, like, hero, films 
 Topic 50: movie, church, smith, joseph, recommend 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 21 (approx. per word bound = -7.445, relative change = 4.422e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 22 (approx. per word bound = -7.445, relative change = 4.272e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 23 (approx. per word bound = -7.444, relative change = 3.972e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 24 (approx. per word bound = -7.444, relative change = 3.983e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 25 (approx. per word bound = -7.444, relative change = 4.210e-05) 
Topic 1: film, characters, script, even, just 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: movie, great, one, film, story 
 Topic 7: movie, film, good, interesting, really 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, &lt;, br, &gt;, family 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, just, people 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, good 
 Topic 19: film, movie, one, much, &gt; 
 Topic 20: match, one, rock, wwe, ring 
 Topic 21: one, movie, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, movie 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, way, like 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, movie, funny 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, movie, film, see 
 Topic 31: good, film, movie, bad, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: movie, film, see, also, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, documentary, like, film, see 
 Topic 36: like, film, one, &gt;, br 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, movie, novel, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, one, role, even 
 Topic 42: film, one, get, real, just 
 Topic 43: movie, one, just, film, like 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: &gt;, br, &lt;, life, movie 
 Topic 49: film, one, like, hero, films 
 Topic 50: movie, church, smith, joseph, film 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 26 (approx. per word bound = -7.444, relative change = 3.630e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 27 (approx. per word bound = -7.443, relative change = 3.169e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 28 (approx. per word bound = -7.443, relative change = 2.985e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 29 (approx. per word bound = -7.443, relative change = 2.568e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 30 (approx. per word bound = -7.443, relative change = 2.562e-05) 
Topic 1: film, characters, script, even, just 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, one 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: movie, great, one, film, story 
 Topic 7: movie, film, good, interesting, really 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, family, &lt;, br, &gt; 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, just, people 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, good 
 Topic 19: film, movie, one, much, like 
 Topic 20: match, one, rock, wwe, back 
 Topic 21: one, movie, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, movie 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, way, comedy 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, movie, funny 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, movie, film, people 
 Topic 31: good, film, bad, movie, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: movie, film, see, also, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, like, documentary, film, see 
 Topic 36: like, film, one, &gt;, br 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, movie, novel, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, one, role, even 
 Topic 42: film, one, get, real, just 
 Topic 43: one, movie, just, film, like 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: life, &gt;, br, &lt;, movie 
 Topic 49: film, one, like, hero, films 
 Topic 50: movie, church, smith, joseph, film 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 31 (approx. per word bound = -7.443, relative change = 2.257e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 32 (approx. per word bound = -7.442, relative change = 2.386e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 33 (approx. per word bound = -7.442, relative change = 2.667e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 34 (approx. per word bound = -7.442, relative change = 2.594e-05) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 35 (approx. per word bound = -7.442, relative change = 2.436e-05) 
Topic 1: film, characters, script, even, just 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, story 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: great, movie, one, film, story 
 Topic 7: movie, film, good, interesting, one 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, family, one, br, &lt; 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, just, people 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, lot 
 Topic 19: film, movie, one, much, like 
 Topic 20: match, one, rock, wwe, back 
 Topic 21: one, movie, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, movie 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, way, comedy 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, funny, movie 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, film, movie, people 
 Topic 31: good, film, bad, movie, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: movie, film, see, also, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, like, film, documentary, see 
 Topic 36: like, film, one, &gt;, br 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, movie, novel, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, role, one, even 
 Topic 42: film, one, get, real, just 
 Topic 43: one, movie, just, film, david 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: life, &gt;, movie, br, &lt; 
 Topic 49: film, one, hero, like, films 
 Topic 50: church, movie, smith, joseph, film 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 36 (approx. per word bound = -7.442, relative change = 2.246e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 37 (approx. per word bound = -7.442, relative change = 2.028e-05) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 38 (approx. per word bound = -7.441, relative change = 1.628e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 39 (approx. per word bound = -7.441, relative change = 2.086e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 40 (approx. per word bound = -7.441, relative change = 1.658e-05) 
Topic 1: film, characters, script, even, just 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, story 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: great, movie, one, film, story 
 Topic 7: film, movie, interesting, good, one 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, family, one, love, br 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, just, people 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, lot 
 Topic 19: film, movie, one, much, like 
 Topic 20: match, one, rock, wwe, back 
 Topic 21: one, movie, movies, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, movie 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, way, comedy 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, funny, movie 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, film, movie, people 
 Topic 31: good, film, bad, movie, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: movie, film, see, also, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, film, like, documentary, see 
 Topic 36: like, film, one, just, game 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, movie, novel, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, role, one, even 
 Topic 42: film, one, get, real, just 
 Topic 43: one, movie, just, film, david 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: &lt;, br, &gt;, film, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: life, movie, film, one, &gt; 
 Topic 49: film, one, hero, like, films 
 Topic 50: church, movie, smith, joseph, film 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 41 (approx. per word bound = -7.441, relative change = 1.891e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 42 (approx. per word bound = -7.441, relative change = 1.799e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 43 (approx. per word bound = -7.441, relative change = 1.672e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 44 (approx. per word bound = -7.441, relative change = 1.742e-05) 
....................................................................................................
Completed E-Step (6 seconds). 
Completed M-Step. 
Completing Iteration 45 (approx. per word bound = -7.440, relative change = 1.596e-05) 
Topic 1: film, characters, script, even, like 
 Topic 2: movie, film, actors, director, one 
 Topic 3: &gt;, &lt;, br, film, story 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: great, movie, one, film, story 
 Topic 7: film, movie, interesting, good, one 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, family, one, love, story 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, people, just 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, lot 
 Topic 19: film, movie, one, much, like 
 Topic 20: match, one, rock, wwe, back 
 Topic 21: one, movies, movie, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, time 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, way, comedy 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, funny, movie 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, film, movie, people 
 Topic 31: good, film, bad, movie, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: film, movie, see, also, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, film, like, documentary, see 
 Topic 36: like, film, one, just, game 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, novel, movie, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, role, one, even 
 Topic 42: film, one, get, real, just 
 Topic 43: one, movie, just, film, david 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: film, &lt;, br, &gt;, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: life, movie, film, one, story 
 Topic 49: film, one, hero, like, films 
 Topic 50: church, movie, smith, joseph, film 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 46 (approx. per word bound = -7.440, relative change = 1.206e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 47 (approx. per word bound = -7.440, relative change = 2.095e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 48 (approx. per word bound = -7.440, relative change = 2.575e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 49 (approx. per word bound = -7.440, relative change = 1.747e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 50 (approx. per word bound = -7.440, relative change = 1.878e-05) 
Topic 1: film, characters, script, even, character 
 Topic 2: movie, film, director, actors, one 
 Topic 3: &gt;, &lt;, br, film, well 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, well 
 Topic 6: great, movie, one, film, story 
 Topic 7: film, movie, interesting, good, one 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, family, one, love, story 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, people, just 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, lot 
 Topic 19: film, movie, one, much, like 
 Topic 20: match, one, rock, wwe, back 
 Topic 21: one, movies, movie, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, time 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, good, way, comedy 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, funny, laugh 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, film, movie, people 
 Topic 31: good, film, bad, movie, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: film, movie, also, see, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, film, like, documentary, see 
 Topic 36: like, film, one, just, game 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, novel, movie, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, role, one, even 
 Topic 42: film, one, get, real, just 
 Topic 43: one, movie, just, film, david 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: film, &lt;, br, &gt;, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: life, movie, film, one, story 
 Topic 49: film, one, hero, films, like 
 Topic 50: movie, church, smith, joseph, film 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 51 (approx. per word bound = -7.440, relative change = 1.428e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 52 (approx. per word bound = -7.440, relative change = 1.561e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 53 (approx. per word bound = -7.439, relative change = 1.422e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 54 (approx. per word bound = -7.439, relative change = 1.317e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 55 (approx. per word bound = -7.439, relative change = 1.148e-05) 
Topic 1: film, characters, script, even, character 
 Topic 2: movie, film, director, actors, one 
 Topic 3: &gt;, &lt;, br, film, well 
 Topic 4: film, excellent, one, story, woman 
 Topic 5: best, character, film, one, great 
 Topic 6: great, movie, one, film, story 
 Topic 7: film, movie, interesting, good, one 
 Topic 8: film, one, think, also, plot 
 Topic 9: film, family, one, love, story 
 Topic 10: film, movie, never, story, got 
 Topic 11: time, good, film, one, actually 
 Topic 12: film, one, life, may, young 
 Topic 13: film, lost, one, films, like 
 Topic 14: movie, see, bad, people, just 
 Topic 15: film, one, much, 2, even 
 Topic 16: movie, film, one, just, like 
 Topic 17: bad, movie, get, one, like 
 Topic 18: movie, like, just, tv, lot 
 Topic 19: film, one, movie, much, like 
 Topic 20: match, one, rock, wwe, back 
 Topic 21: one, movies, movie, film, seen 
 Topic 22: film, just, time, one, like 
 Topic 23: new, one, film, joe, time 
 Topic 24: movie, love, film, s, one 
 Topic 25: &gt;, br, &lt;, movie, now 
 Topic 26: film, can, way, comedy, good 
 Topic 27: series, original, film, one, like 
 Topic 28: one, comedy, film, funny, laugh 
 Topic 29: film, films, one, best, also 
 Topic 30: one, like, film, movie, people 
 Topic 31: good, film, bad, movie, scenes 
 Topic 32: movie, just, like, really, one 
 Topic 33: film, movie, also, see, good 
 Topic 34: film, horror, one, just, films 
 Topic 35: people, film, like, documentary, see 
 Topic 36: film, like, one, just, game 
 Topic 37: movie, director, film, good, one 
 Topic 38: show, episode, one, like, episodes 
 Topic 39: book, novel, movie, read, film 
 Topic 40: film, show, like, one, even 
 Topic 41: film, story, role, one, acting 
 Topic 42: film, one, get, real, just 
 Topic 43: one, movie, just, film, david 
 Topic 44: show, one, film, stage, can 
 Topic 45: film, one, heart, make, story 
 Topic 46: film, &lt;, br, &gt;, one 
 Topic 47: film, one, man, cast, many 
 Topic 48: life, movie, film, one, story 
 Topic 49: film, one, hero, films, like 
 Topic 50: movie, church, smith, people, joseph 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 56 (approx. per word bound = -7.439, relative change = 1.079e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Completing Iteration 57 (approx. per word bound = -7.439, relative change = 1.084e-05) 
....................................................................................................
Completed E-Step (5 seconds). 
Completed M-Step. 
Model Converged </code></pre>
</div>
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(differentKs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Tutorial10_files/figure-html/unnamed-chunk-42-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The plot is a mixed bag for us. Higher values of the held-out likelihood and semantic coherence both indicate better models, while lower values of residuals indicates a better model. It’s also important to note that it’s artificially easy to get more semantic coherence by having fewer topics (semantic coherence is a measure based on how well the top topic words identify the topics). If it was me, I’d probably settle at the midpoint here (25 topics). But there’s no magic solution. Instead, the decision is largely left up to you. That flexibility is nice, but it also means that *you need to be able to defend your choice of K**, because external audiences are going to want to know why you chose the number you did.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>